<!DOCTYPE html>
<!-- saved from url=(0414)https://disqus.com/embed/comments/?base=default&f=kratzertblog&t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&s_o=default#version=594a21d2f28c61c5869ee69d5e22040f -->
<html lang="en" dir="ltr" class="js no-touch localstorage sessionstorage contenteditable use-opacity-transitions"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Disqus Comments</title>

    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <style>
        .alert--warning {
            border-radius: 3px;
            padding: 10px 15px;
            margin-bottom: 10px;
            background-color: #FFE070;
            color: #A47703;
        }

        .alert--warning a,
        .alert--warning a:hover,
        .alert--warning strong {
            color: #A47703;
            font-weight: bold;
        }

        .alert--error p,
        .alert--warning p {
            margin-top: 5px;
            margin-bottom: 5px;
        }
        
        </style>
    
    <style>
        
        html {
            overflow: hidden;
        }
        

        #error {
            display: none;
        }

        .clearfix:after {
            content: "";
            display: block;
            height: 0;
            clear: both;
            visibility: hidden;
        }

        
    </style>

<script src="./cb=gapi.loaded_0" async=""></script><script src="./sdk.js.다운로드" async="" crossorigin="anonymous"></script><script crossorigin="anonymous" id="bootstrap-script" data-app="lounge" src="./lounge.load.594a21d2f28c61c5869ee69d5e22040f.js.다운로드"></script><meta http-equiv="Content-Security-Policy" content="script-src https:;"><link rel="stylesheet" href="./lounge.90bbe7dd462e64a6c99045d2dadef75c.css"><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="lounge/main" src="./lounge.bundle.7fb8dd2e46641c9b2df6d6bf3faf0262.js.다운로드"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="remote/config" src="./config.js.다운로드"></script><style id="css_1563363070594"></style><!--<base target="_parent">--><base href="." target="_parent"><link rel="stylesheet" href="./discovery.539723090a04fba1b162858415868357.css"><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="discovery/main" src="./discovery.bundle.05463248c2ace681de05d193251bc001.js.다운로드"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="fb" src="./sdk.js(1).다운로드"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="gapi" src="./api.js.다운로드" gapi_processed="true"></script><style type="text/css">.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:"lucida grande", tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}@keyframes fb_transform{from{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}
.fb_dialog{background:rgba(82, 82, 82, .7);position:absolute;top:-10000px;z-index:10001}.fb_dialog_advanced{border-radius:8px;padding:10px}.fb_dialog_content{background:#fff;color:#373737}.fb_dialog_close_icon{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 0 transparent;cursor:pointer;display:block;height:15px;position:absolute;right:18px;top:17px;width:15px}.fb_dialog_mobile .fb_dialog_close_icon{left:5px;right:auto;top:5px}.fb_dialog_padding{background-color:transparent;position:absolute;width:1px;z-index:-1}.fb_dialog_close_icon:hover{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -15px transparent}.fb_dialog_close_icon:active{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -30px transparent}.fb_dialog_iframe{line-height:0}.fb_dialog_content .dialog_title{background:#6d84b4;border:1px solid #365899;color:#fff;font-size:14px;font-weight:bold;margin:0}.fb_dialog_content .dialog_title>span{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yd/r/Cou7n-nqK52.gif) no-repeat 5px 50%;float:left;padding:5px 0 7px 26px}body.fb_hidden{height:100%;left:0;margin:0;overflow:visible;position:absolute;top:-10000px;transform:none;width:100%}.fb_dialog.fb_dialog_mobile.loading{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/ya/r/3rhSv5V8j3o.gif) white no-repeat 50% 50%;min-height:100%;min-width:100%;overflow:hidden;position:absolute;top:0;z-index:10001}.fb_dialog.fb_dialog_mobile.loading.centered{background:none;height:auto;min-height:initial;min-width:initial;width:auto}.fb_dialog.fb_dialog_mobile.loading.centered #fb_dialog_loader_spinner{width:100%}.fb_dialog.fb_dialog_mobile.loading.centered .fb_dialog_content{background:none}.loading.centered #fb_dialog_loader_close{clear:both;color:#fff;display:block;font-size:18px;padding-top:20px}#fb-root #fb_dialog_ipad_overlay{background:rgba(0, 0, 0, .4);bottom:0;left:0;min-height:100%;position:absolute;right:0;top:0;width:100%;z-index:10000}#fb-root #fb_dialog_ipad_overlay.hidden{display:none}.fb_dialog.fb_dialog_mobile.loading iframe{visibility:hidden}.fb_dialog_mobile .fb_dialog_iframe{position:sticky;top:0}.fb_dialog_content .dialog_header{background:linear-gradient(from(#738aba), to(#2c4987));border-bottom:1px solid;border-color:#1d3c78;box-shadow:white 0 1px 1px -1px inset;color:#fff;font:bold 14px Helvetica, sans-serif;text-overflow:ellipsis;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0;vertical-align:middle;white-space:nowrap}.fb_dialog_content .dialog_header table{height:43px;width:100%}.fb_dialog_content .dialog_header td.header_left{font-size:12px;padding-left:5px;vertical-align:middle;width:60px}.fb_dialog_content .dialog_header td.header_right{font-size:12px;padding-right:5px;vertical-align:middle;width:60px}.fb_dialog_content .touchable_button{background:linear-gradient(from(#4267B2), to(#2a4887));background-clip:padding-box;border:1px solid #29487d;border-radius:3px;display:inline-block;line-height:18px;margin-top:3px;max-width:85px;padding:4px 12px;position:relative}.fb_dialog_content .dialog_header .touchable_button input{background:none;border:none;color:#fff;font:bold 12px Helvetica, sans-serif;margin:2px -12px;padding:2px 6px 3px 6px;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog_content .dialog_header .header_center{color:#fff;font-size:16px;font-weight:bold;line-height:18px;text-align:center;vertical-align:middle}.fb_dialog_content .dialog_content{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/y9/r/jKEcVPZFk-2.gif) no-repeat 50% 50%;border:1px solid #4a4a4a;border-bottom:0;border-top:0;height:150px}.fb_dialog_content .dialog_footer{background:#f5f6f7;border:1px solid #4a4a4a;border-top-color:#ccc;height:40px}#fb_dialog_loader_close{float:left}.fb_dialog.fb_dialog_mobile .fb_dialog_close_button{text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog.fb_dialog_mobile .fb_dialog_close_icon{visibility:hidden}#fb_dialog_loader_spinner{animation:rotateSpinner 1.2s linear infinite;background-color:transparent;background-image:url(https://static.xx.fbcdn.net/rsrc.php/v3/yD/r/t-wz8gw1xG1.png);background-position:50% 50%;background-repeat:no-repeat;height:24px;width:24px}@keyframes rotateSpinner{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}
.fb_iframe_widget{display:inline-block;position:relative}.fb_iframe_widget span{display:inline-block;position:relative;text-align:justify}.fb_iframe_widget iframe{position:absolute}.fb_iframe_widget_fluid_desktop,.fb_iframe_widget_fluid_desktop span,.fb_iframe_widget_fluid_desktop iframe{max-width:100%}.fb_iframe_widget_fluid_desktop iframe{min-width:220px;position:relative}.fb_iframe_widget_lift{z-index:1}.fb_iframe_widget_fluid{display:inline}.fb_iframe_widget_fluid span{width:100%}</style></head>
<body class="serif">
    

    
    <div id="error" class="alert--error">
        <p>We were unable to load Disqus. If you are a moderator please see our <a href="https://docs.disqus.com/help/83/"> troubleshooting guide</a>. </p>
    </div>

    
    <script type="text/json" id="disqus-forumData">{"session":{"canModerate":false,"audienceSyncVerified":false,"mustVerify":false,"canReply":true,"mustVerifyEmail":false},"forum":{"aetBannerConfirmation":null,"founder":"196251915","twitterName":null,"commentsLinkOne":"1 Comment","guidelines":null,"favicon":{"permalink":"https://disqus.com/api/forums/favicons/kratzertblog.jpg","cache":"//a.disquscdn.com/1561077851/images/favicon-default.png"},"commentsLinkZero":"0 Comments","disableDisqusBranding":false,"daysAlive":0,"createdAt":"2016-02-12T06:01:19.484948","category":"Tech","aetBannerEnabled":false,"aetBannerTitle":null,"raw_guidelines":null,"colorScheme":"auto","id":"kratzertblog","installCompleted":true,"moderatorBadgeText":"","commentPolicyText":null,"aetEnabled":false,"channel":null,"sort":4,"description":null,"newPolicy":true,"raw_description":null,"language":"en","adsReviewStatus":0,"pk":"4023296","forumCategory":{"date_added":"2016-01-28T01:54:31","id":8,"name":"Tech"},"permissions":{},"commentPolicyLink":null,"aetBannerDescription":null,"name":"kratzertblog","commentsLinkMultiple":"{num} Comments","settings":{"threadRatingsEnabled":false,"adsDRNativeEnabled":false,"disable3rdPartyTrackers":false,"adsVideoEnabled":false,"allowAnonPost":false,"threadRatingsEnabledByDefault":true,"audienceSyncEnabled":false,"unapproveLinks":false,"linkAffiliationEnabled":true,"adsProductLinksThumbnailsEnabled":false,"adsProductStoriesEnabled":false,"organicDiscoveryEnabled":true,"adsProductDisplayEnabled":false,"adsProductLinksEnabled":false,"threadReactionsEnabled":false,"adsEnabled":false,"adsPositionTopEnabled":false,"hasCustomAvatar":false,"adultContent":false,"allowAnonVotes":false,"adsProductVideoEnabled":false,"mustVerify":true,"gifPickerEnabled":true,"mustVerifyEmail":true,"ssoRequired":false,"mediaembedEnabled":true,"adsPositionBottomEnabled":false,"discoveryLocked":false,"validateAllPosts":false,"adsSettingsLocked":false,"isVIP":false,"adsPositionInthreadEnabled":false},"organizationId":2907269,"typeface":"auto","url":"","daysThreadAlive":0,"avatar":{"small":{"permalink":"https://disqus.com/api/forums/avatars/kratzertblog.jpg?size=32","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"large":{"permalink":"https://disqus.com/api/forums/avatars/kratzertblog.jpg?size=92","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}},"signedUrl":""}}</script>

    <script type="text/json" id="disqus-threadData">{"cursor":{"hasPrev":false,"prev":null,"total":119,"hasNext":true,"next":"1:0:0"},"code":0,"response":{"lastModified":1563280886,"posts":[{"editableUntil":"2017-12-06T21:19:10","dislikes":0,"numReports":0,"likes":3,"message":"\u003cp>Thanks ! the flowchart help me so much to implement in R ;-)\u003cbr>\u003ca href=\"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg\" rel=\"nofollow noopener\" title=\"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg\">https://uploads.disquscdn.c...\u003c/a>\u003c/p>","id":"3638799277","createdAt":"2017-11-29T21:19:10","author":{"username":"disqus_cxY7uKUvpl","about":"","name":"aboul","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-11-29T21:09:14","profileUrl":"https://disqus.com/by/disqus_cxY7uKUvpl/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"272639134","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_cxY7uKUvpl.jpg","cache":"https://c.disquscdn.com/uploads/users/27263/9134/avatar32.jpg?1511990351"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/disqus_cxY7uKUvpl.jpg","cache":"https://c.disquscdn.com/uploads/users/27263/9134/avatar92.jpg?1511990351","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_cxY7uKUvpl.jpg","cache":"https://c.disquscdn.com/uploads/users/27263/9134/avatar92.jpg?1511990351"}}},"media":[{"providerName":"Disquscdn","resolvedUrl":"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","thumbnailUrl":"//uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","htmlHeight":null,"id":34057645,"thumbnailWidth":3192,"title":"","htmlWidth":null,"mediaType":"2","html":"","location":"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","type":"5","metadata":{"create_method":"preview","thumbnail":"//uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg"},"urlRedirect":"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","description":"","post":"3638799277","thumbnailURL":"//uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","thread":"4617794568","forum":"kratzertblog","url":"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","resolvedUrlRedirect":"https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","thumbnailHeight":2376}],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thanks ! the flowchart help me so much to implement in R ;-)\nhttps://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":3,"moderationLabels":["media"],"isEdited":false,"sb":false},{"editableUntil":"2017-12-07T08:58:30","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Looks like my sketches, when I first sat down to derive the graph for my self. Good work!\u003c/p>","id":"3639479740","createdAt":"2017-11-30T08:58:30","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3638799277,"isApproved":true,"isFlagged":false,"raw_message":"Looks like my sketches, when I first sat down to derive the graph for my self. Good work!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-01-02T07:18:47","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hi Frederik,\u003c/p>\u003cp>Thanks for the great explanation in this post, but I had some trouble understanding the backprop in step 9, specifically why an np.sum() is used rather than np.mean() to calculate dbeta. My intuition is, during the addition step with numpy broadcasting to every row, any error in beta is duplicated n_row times, and thus dbeta should be the average across rows of dout.\u003c/p>\u003cp>From what I can see in aboul's handwriting in the flow chart above, he used average rather than sum. I was wondering if you could shed some light onto this issue. Thanks!\u003c/p>\u003cp>Patrick\u003c/p>\u003cp>==========\u003cbr>Edit: Never mind. I found the explanation on the link you gave at the end of the post \u003ca href=\"http://disq.us/url?url=http%3A%2F%2Fcthorey.github.io%2Fbackpropagation%2F%3Ar33DVWl8q8rDRF9zRhyrL5527fg&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"http://cthorey.github.io/backpropagation/\">http://cthorey.github.io/ba...\u003c/a> in the section \"We can therefore chain the gradient of the loss with respect to the input h_ij by the gradient of the loss with respect to ALL the outputs y_kl which reads ...\". This is related to the notion of \"total derivative\" \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTotal_derivative%3AdO1EtjQujaspBuHilHbsqfsJopg&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"https://en.wikipedia.org/wiki/Total_derivative\">https://en.wikipedia.org/wi...\u003c/a>.\u003c/p>\u003cp>In addition, I agree with John McIain's explanation above that a more appropriate explanation about why the broadcast items are added up but are not averaged is that the final cost is the summed across different rows,\u003c/p>","id":"3678638435","createdAt":"2017-12-26T07:18:47","author":{"username":"patricklangechuanliu","about":"","name":"Patrick Langechuan LIU","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-12-26T07:12:30","profileUrl":"https://disqus.com/by/patricklangechuanliu/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"275100268","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/patricklangechuanliu.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/patricklangechuanliu.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/patricklangechuanliu.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3639479740,"isApproved":true,"isFlagged":false,"raw_message":"Hi Frederik,\n\nThanks for the great explanation in this post, but I had some trouble understanding the backprop in step 9, specifically why an np.sum() is used rather than np.mean() to calculate dbeta. My intuition is, during the addition step with numpy broadcasting to every row, any error in beta is duplicated n_row times, and thus dbeta should be the average across rows of dout. \n\nFrom what I can see in aboul's handwriting in the flow chart above, he used average rather than sum. I was wondering if you could shed some light onto this issue. Thanks!\n\nPatrick\n\n==========\nEdit: Never mind. I found the explanation on the link you gave at the end of the post http://cthorey.github.io/backpropagation/ in the section \"We can therefore chain the gradient of the loss with respect to the input h_ij by the gradient of the loss with respect to ALL the outputs y_kl which reads ...\". This is related to the notion of \"total derivative\" https://en.wikipedia.org/wiki/Total_derivative.\n\nIn addition, I agree with John McIain's explanation above that a more appropriate explanation about why the broadcast items are added up but are not averaged is that the final cost is the summed across different rows,","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":0,"moderationLabels":["links"],"isEdited":true,"sb":false},{"editableUntil":"2019-03-06T00:44:23","dislikes":0,"numReports":0,"likes":2,"message":"\u003cp>Hi Frederik,\u003c/p>\u003cp>I'm having trouble understanding your derivation for dBeta and dGamma. Why do you take the sum of the gradient over all the backprop inputs for that node? Shouldn't it be the average of all the backprop inputs for that node?\u003c/p>\u003cp>To my knowledge, that's how dTheta terms are calculated in other layers of the neural network as well.\u003c/p>","id":"4355771341","createdAt":"2019-02-27T00:44:23","author":{"username":"disqus_OcxfWmpedA","about":"","name":"Rahul Devanarayanan","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2013-02-13T07:14:28","profileUrl":"https://disqus.com/by/disqus_OcxfWmpedA/","url":"https://www.facebook.com/app_scoped_user_id/YXNpZADpBWEh4V0NrZAWMwZAGpKTGM4cHhBSlhFVGliRk5YUTNJaTFUOERSdnR0X0txM0t1bGxqQU1McFlvOVc4akRZAcU9hMXdZAbDNpeGtOREpoVWFUSVVqU1o0STZACckNRbmZAlUGpTN3Qw/","location":"","isPrivate":false,"signedUrl":"https://disq.us/?url=https%3A%2F%2Fwww.facebook.com%2Fapp_scoped_user_id%2FYXNpZADpBWEh4V0NrZAWMwZAGpKTGM4cHhBSlhFVGliRk5YUTNJaTFUOERSdnR0X0txM0t1bGxqQU1McFlvOVc4akRZAcU9hMXdZAbDNpeGtOREpoVWFUSVVqU1o0STZACckNRbmZAlUGpTN3Qw%2F&key=fBB7Wv4gINWVOecpsNarAA","isPrimary":true,"isAnonymous":false,"id":"43571292","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_OcxfWmpedA.jpg","cache":"https://c.disquscdn.com/uploads/users/4357/1292/avatar32.jpg?1551228264"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/disqus_OcxfWmpedA.jpg","cache":"https://c.disquscdn.com/uploads/users/4357/1292/avatar92.jpg?1551228264","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_OcxfWmpedA.jpg","cache":"https://c.disquscdn.com/uploads/users/4357/1292/avatar92.jpg?1551228264"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Hi Frederik,\n\nI'm having trouble understanding your derivation for dBeta and dGamma. Why do you take the sum of the gradient over all the backprop inputs for that node? Shouldn't it be the average of all the backprop inputs for that node?\n\nTo my knowledge, that's how dTheta terms are calculated in other layers of the neural network as well.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":2,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-01-01T15:28:27","dislikes":0,"numReports":0,"likes":2,"message":"\u003cp>hi\uff0c\u003cbr>I believe there is something wrong with your explanation about why the broadcast terms are added up. For example beta are summed in columns.\u003c/p>\u003cp>\"And because the summation of beta during the forward pass is a row-wise summation, during the backward pass we need to sum up the gradient over all of its columns (take a look at the dimensions).\"\u003c/p>\u003cp>I think the reason is that in the end loss different training examples are summed for the final loss evaluation. Not the reason you give.\u003c/p>","id":"3677924058","createdAt":"2017-12-25T15:28:27","author":{"username":"disqus_lDJKDl7VxM","about":"","name":"JohnMclain","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-12-25T15:24:57","profileUrl":"https://disqus.com/by/disqus_lDJKDl7VxM/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"275048753","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_lDJKDl7VxM.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/disqus_lDJKDl7VxM.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_lDJKDl7VxM.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"hi\uff0c\nI believe there is something wrong with your explanation about why the broadcast terms are added up. For example beta are summed in columns.\n\n\"And because the summation of beta during the forward pass is a row-wise summation, during the backward pass we need to sum up the gradient over all of its columns (take a look at the dimensions).\"\n\nI think the reason is that in the end loss different training examples are summed for the final loss evaluation. Not the reason you give.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":2,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2018-01-22T08:40:37","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Sorry John for replying so late, I somehow haven't seen your post but was now directed through another comment to your comment again. And well I think you are totally right (I adapted the passages in step 9 and 8 of the backprop, you might take a look and give me your opinion on the updated version).\u003c/p>","id":"3708575158","createdAt":"2018-01-15T08:40:37","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3677924058,"isApproved":true,"isFlagged":false,"raw_message":"Sorry John for replying so late, I somehow haven't seen your post but was now directed through another comment to your comment again. And well I think you are totally right (I adapted the passages in step 9 and 8 of the backprop, you might take a look and give me your opinion on the updated version).","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-07T09:04:18","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hi, I have been struggling with getting this part for a while. I ended up writing out the full expression of dL/d\\beta (or any broadcasting part) to arrive at the same result (quite some work for one step, phew).\u003c/p>\u003cp>I think the reason for adding up rows is the multivariate-calculus rule which says \"gradients flow into different branches of the graph should be summed\". Even if the loss function at the end is not a sum, we still should sum the gradient from different branches that flow into one variable.\u003c/p>\u003cp>As a concrete example, in step 9, a term of dL/d\\beta should be:\u003cbr> \u003ca href=\"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif\" rel=\"nofollow noopener\" title=\"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif\">https://uploads.disquscdn.c...\u003c/a> \u003cbr>where z_tilde is the output in your notation. Obviously only the i-th column of the output is involved in the forward path so this derivative is the sum of the i-th column of dout.\u003c/p>\u003cp>However, I do have a question. I found myself in need of doing this full-out derivation basically every time which is quite non-trivial. What's your thinking track that leads to the expression without all the element-wise writting-out?\u003c/p>","id":"3879045238","createdAt":"2018-04-30T09:04:18","author":{"username":"harveyqiu","about":"","name":"Harvey Qiu","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-04-30T08:56:20","profileUrl":"https://disqus.com/by/harveyqiu/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"286645466","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/harveyqiu.jpg","cache":"https://c.disquscdn.com/uploads/users/28664/5466/avatar32.jpg?1525078584"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/harveyqiu.jpg","cache":"https://c.disquscdn.com/uploads/users/28664/5466/avatar92.jpg?1525078584","large":{"permalink":"https://disqus.com/api/users/avatars/harveyqiu.jpg","cache":"https://c.disquscdn.com/uploads/users/28664/5466/avatar92.jpg?1525078584"}}},"media":[{"providerName":"Disquscdn","resolvedUrl":"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","thumbnailUrl":"//uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","htmlHeight":null,"id":40228045,"thumbnailWidth":171,"title":"","htmlWidth":null,"mediaType":"2","html":"","location":"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","type":"5","metadata":{"create_method":"preview","thumbnail":"//uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif"},"urlRedirect":"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","description":"","post":"3879045238","thumbnailURL":"//uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","thread":"4617794568","forum":"kratzertblog","url":"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","resolvedUrlRedirect":"https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif","thumbnailHeight":49}],"isSpam":false,"hasMore":true,"isDeleted":false,"isDeletedByAuthor":false,"parent":3708575158,"isApproved":true,"isFlagged":false,"raw_message":"Hi, I have been struggling with getting this part for a while. I ended up writing out the full expression of dL/d\\beta (or any broadcasting part) to arrive at the same result (quite some work for one step, phew). \n\nI think the reason for adding up rows is the multivariate-calculus rule which says \"gradients flow into different branches of the graph should be summed\". Even if the loss function at the end is not a sum, we still should sum the gradient from different branches that flow into one variable. \n\nAs a concrete example, in step 9, a term of dL/d\\beta should be:\n https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif \nwhere z_tilde is the output in your notation. Obviously only the i-th column of the output is involved in the forward path so this derivative is the sum of the i-th column of dout.\n\nHowever, I do have a question. I found myself in need of doing this full-out derivation basically every time which is quite non-trivial. What's your thinking track that leads to the expression without all the element-wise writting-out?","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2018-03-02T18:41:11","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Thanks fkratzert for your effort !!\u003cbr>I verified your code passes Andrew NG's Gradient Checking.\u003cbr>Cheers!!\u003c/p>","id":"3772850279","createdAt":"2018-02-23T18:41:11","author":{"username":"disqus_GJH1JRIogE","about":"","name":"Jorge","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-02-23T18:24:37","profileUrl":"https://disqus.com/by/disqus_GJH1JRIogE/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"280979018","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_GJH1JRIogE.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/disqus_GJH1JRIogE.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_GJH1JRIogE.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thanks fkratzert for your effort !!\nI verified your code passes Andrew NG's Gradient Checking.\nCheers!!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-03-02T19:00:47","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Thanks man, good to hear ;)\u003c/p>","id":"3772882856","createdAt":"2018-02-23T19:00:47","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3772850279,"isApproved":true,"isFlagged":false,"raw_message":"Thanks man, good to hear ;)","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-29T16:34:25","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Thank you! You have a knack of simplifying complexity :-)\u003c/p>","id":"3579296064","createdAt":"2017-10-22T16:34:25","author":{"username":"anandsaha","about":"","name":"Anand Saha","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2011-02-08T03:40:38","profileUrl":"https://disqus.com/by/anandsaha/","url":"http://teleported.in","location":"Pune, India","isPrivate":true,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fteleported.in&key=h4uu9ADYgxyWuK2xjbQSQQ","isPrimary":true,"isAnonymous":false,"id":"7128646","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/anandsaha.jpg","cache":"https://c.disquscdn.com/uploads/users/712/8646/avatar32.jpg?1493544799"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/anandsaha.jpg","cache":"https://c.disquscdn.com/uploads/users/712/8646/avatar92.jpg?1493544799","large":{"permalink":"https://disqus.com/api/users/avatars/anandsaha.jpg","cache":"https://c.disquscdn.com/uploads/users/712/8646/avatar92.jpg?1493544799"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thank you! You have a knack of simplifying complexity :-)","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-30T07:16:13","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks, this is really what I tried. I had a hard time to work myself through this topic, back when I followed the cs231n online and set many hours in front of a paper an tried to derive myself everything on my own. At the end I broke everything down to this really simple steps.\u003c/p>","id":"3580214271","createdAt":"2017-10-23T07:16:13","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3579296064,"isApproved":true,"isFlagged":false,"raw_message":"Thanks, this is really what I tried. I had a hard time to work myself through this topic, back when I followed the cs231n online and set many hours in front of a paper an tried to derive myself everything on my own. At the end I broke everything down to this really simple steps.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-07-23T12:36:01","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>But still wrong. Still not give a precise explanation about the sigma of beta term.\u003c/p>","id":"4541606339","createdAt":"2019-07-16T12:36:01","author":{"username":"disqus_UXlWQroB4n","about":"","name":"\u79b9\u90b5 \u8cf4","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-09-22T06:56:02","profileUrl":"https://disqus.com/by/disqus_UXlWQroB4n/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"122867415","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_UXlWQroB4n.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/disqus_UXlWQroB4n.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_UXlWQroB4n.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3580214271,"isApproved":true,"isFlagged":false,"raw_message":"But still wrong. Still not give a precise explanation about the sigma of beta term.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2017-10-18T07:15:25","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Thanks. I got the intuition of how to derive the derivative of  np.sum along one axis.\u003c/p>","id":"3561820127","createdAt":"2017-10-11T07:15:25","author":{"username":"majicji","about":"","name":"Majic Ji","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-10-11T07:15:12","profileUrl":"https://disqus.com/by/majicji/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"267800662","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/majicji.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/majicji.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/majicji.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thanks. I got the intuition of how to derive the derivative of  np.sum along one axis.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-29T08:31:07","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Glad I could help you.\u003c/p>","id":"3578840984","createdAt":"2017-10-22T08:31:07","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3561820127,"isApproved":true,"isFlagged":false,"raw_message":"Glad I could help you.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-04-29T02:49:36","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Really great post.  Once you have calculated the backward gradients, are they immediately subtracted from the gamma and beta terms to update them before forward propogating the next training batch?  And once all minibatches are completed, are these terms reset to 1 and 0 before the next epoch?  I am thinking of cases where each complete forward and backward training epoch is self-contained and independent from other epochs, returning only the updated weights with each pass.  I'm also wondering how to implement batch normalization for testing when the gamma and beta from training is unknown.\u003cbr>Thanks!\u003c/p>","id":"3268047638","createdAt":"2017-04-22T02:49:36","author":{"username":"jschaeff","about":"","name":"jschaeff","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-04-22T02:17:36","profileUrl":"https://disqus.com/by/jschaeff/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"249599616","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Really great post.  Once you have calculated the backward gradients, are they immediately subtracted from the gamma and beta terms to update them before forward propogating the next training batch?  And once all minibatches are completed, are these terms reset to 1 and 0 before the next epoch?  I am thinking of cases where each complete forward and backward training epoch is self-contained and independent from other epochs, returning only the updated weights with each pass.  I'm also wondering how to implement batch normalization for testing when the gamma and beta from training is unknown.\nThanks!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-04-29T14:23:28","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Regarding your first questions: Yes you update all variables of the network immediately, when you have computed the gradients. What would be the purpose of passing another batch through the network without updating the knowledge gained from the last batch into the network? \u003cbr>And regarding your second question: No you don't reset gamma and beta after each epoch. It's the same as for all the other parameters, which you don't reset after each epoch. Remember that you are updating parameters with gradients and a learning rate, so you only take small steps to the direction of a minimum.\u003c/p>\u003cp>And for the last, I don't know if I understand you correctly. You want to apply BatchNorm to a already trained network, that you didn't trained with BatchNorm? What do you expect what will happen?\u003c/p>","id":"3268555117","createdAt":"2017-04-22T14:23:28","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3268047638,"isApproved":true,"isFlagged":false,"raw_message":"Regarding your first questions: Yes you update all variables of the network immediately, when you have computed the gradients. What would be the purpose of passing another batch through the network without updating the knowledge gained from the last batch into the network? \nAnd regarding your second question: No you don't reset gamma and beta after each epoch. It's the same as for all the other parameters, which you don't reset after each epoch. Remember that you are updating parameters with gradients and a learning rate, so you only take small steps to the direction of a minimum. \n\nAnd for the last, I don't know if I understand you correctly. You want to apply BatchNorm to a already trained network, that you didn't trained with BatchNorm? What do you expect what will happen?","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-04-29T15:22:44","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Yes, my network wasn't converging if updating gamma and beta, but I realize now, after your reply, that I was simply subtracting the dgamma and dbeta terms without considering the network learning rate.  Thank you!!!\u003c/p>\u003cp>For the last question, I was unclear.  Say you download the weights for a network, pretrained with BN.  Shouldn't the BN be included also at test time in inference mode, as a deterministic transform?  According to the Ioffe paper, this linear transform uses gamma and beta, along with the training population running means and variances.  My question pertains to when the training data is unavailable.  The training means and variances could possibly be approximated from a complete forward pass with the testing data, using the gamma and beta - but what to do when you don't have these terms?\u003c/p>\u003cp>Thanks again\u003c/p>","id":"3268622523","createdAt":"2017-04-22T15:22:44","author":{"username":"jschaeff","about":"","name":"jschaeff","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-04-22T02:17:36","profileUrl":"https://disqus.com/by/jschaeff/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"249599616","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/jschaeff.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":true,"isDeleted":false,"isDeletedByAuthor":false,"parent":3268555117,"isApproved":true,"isFlagged":false,"raw_message":"Yes, my network wasn't converging if updating gamma and beta, but I realize now, after your reply, that I was simply subtracting the dgamma and dbeta terms without considering the network learning rate.  Thank you!!!\n\nFor the last question, I was unclear.  Say you download the weights for a network, pretrained with BN.  Shouldn't the BN be included also at test time in inference mode, as a deterministic transform?  According to the Ioffe paper, this linear transform uses gamma and beta, along with the training population running means and variances.  My question pertains to when the training data is unavailable.  The training means and variances could possibly be approximated from a complete forward pass with the testing data, using the gamma and beta - but what to do when you don't have these terms?\n\nThanks again","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-05-03T20:01:41","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Great blog, doing gods work, thank you for such a clear explanation.\u003c/p>","id":"4439404565","createdAt":"2019-04-26T20:01:41","author":{"username":"dhawal_gupta","about":"","name":"Dhawal Gupta","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2019-04-26T20:01:08","profileUrl":"https://disqus.com/by/dhawal_gupta/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"331836313","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/dhawal_gupta.jpg","cache":"https://c.disquscdn.com/uploads/users/33183/6313/avatar32.jpg?1556657299"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/dhawal_gupta.jpg","cache":"https://c.disquscdn.com/uploads/users/33183/6313/avatar92.jpg?1556657299","large":{"permalink":"https://disqus.com/api/users/avatars/dhawal_gupta.jpg","cache":"https://c.disquscdn.com/uploads/users/33183/6313/avatar92.jpg?1556657299"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Great blog, doing gods work, thank you for such a clear explanation.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-02-17T18:47:57","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Good one! \u003cbr>Is there a video version of this blog in cs231n? If so can you link the specific video?\u003c/p>","id":"4331155064","createdAt":"2019-02-10T18:47:57","author":{"username":"karthikeyanmg","about":"","name":"Karthikeyan Mg","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-04-07T11:14:32","profileUrl":"https://disqus.com/by/karthikeyanmg/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"102110915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/karthikeyanmg.jpg","cache":"https://c.disquscdn.com/uploads/users/10211/915/avatar32.jpg?1555614682"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/karthikeyanmg.jpg","cache":"https://c.disquscdn.com/uploads/users/10211/915/avatar92.jpg?1555614682","large":{"permalink":"https://disqus.com/api/users/avatars/karthikeyanmg.jpg","cache":"https://c.disquscdn.com/uploads/users/10211/915/avatar92.jpg?1555614682"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Good one! \nIs there a video version of this blog in cs231n? If so can you link the specific video?","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-02-17T19:05:32","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>You mean if they explain it in class somewhere? No they don't, at least not when I did the class by watching the videos. There is one point where Andrej explains chain rule and computational graphs, but never the backpass through the batchnorm layer\u003c/p>","id":"4331176687","createdAt":"2019-02-10T19:05:32","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4331155064,"isApproved":true,"isFlagged":false,"raw_message":"You mean if they explain it in class somewhere? No they don't, at least not when I did the class by watching the videos. There is one point where Andrej explains chain rule and computational graphs, but never the backpass through the batchnorm layer","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-18T06:09:07","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Nice writeup. :)\u003c/p>","id":"3895466348","createdAt":"2018-05-11T06:09:07","author":{"username":"disqus_dVXj63cxog","about":"","name":"Michael Green","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-01-10T20:19:08","profileUrl":"https://disqus.com/by/disqus_dVXj63cxog/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"238245013","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_dVXj63cxog.jpg","cache":"https://c.disquscdn.com/uploads/users/23824/5013/avatar32.jpg?1545925392"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/disqus_dVXj63cxog.jpg","cache":"https://c.disquscdn.com/uploads/users/23824/5013/avatar92.jpg?1545925392","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_dVXj63cxog.jpg","cache":"https://c.disquscdn.com/uploads/users/23824/5013/avatar92.jpg?1545925392"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Nice writeup. :)","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-18T07:52:44","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks :)\u003c/p>","id":"3895523131","createdAt":"2018-05-11T07:52:44","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3895466348,"isApproved":true,"isFlagged":false,"raw_message":"Thanks :)","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-03T18:53:42","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks for the post I loved it :D\u003c/p>","id":"3874235827","createdAt":"2018-04-26T18:53:42","author":{"username":"abhisheknadgeri","about":"","name":"Abhishek Nadgeri","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-04-26T18:53:35","profileUrl":"https://disqus.com/by/abhisheknadgeri/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"286384918","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/abhisheknadgeri.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/abhisheknadgeri.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/abhisheknadgeri.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thanks for the post I loved it :D","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-14T06:37:23","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>you are welcome. great that you liked it!\u003c/p>","id":"3889199453","createdAt":"2018-05-07T06:37:23","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3874235827,"isApproved":true,"isFlagged":false,"raw_message":"you are welcome. great that you liked it!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-04-13T20:42:11","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks for the nice explanation. A small terminological suggestion: when you discuss the derivative /gradient of a function, it is widely accepted to use the term \"derivative\", whereas \"derivation\" instead refers to the process of arriving at a result.\u003c/p>","id":"3842539330","createdAt":"2018-04-06T20:42:11","author":{"username":"ryanneph","about":"","name":"Ryan Neph","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-04-06T20:41:55","profileUrl":"https://disqus.com/by/ryanneph/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"284757841","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/ryanneph.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/ryanneph.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/ryanneph.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thanks for the nice explanation. A small terminological suggestion: when you discuss the derivative /gradient of a function, it is widely accepted to use the term \"derivative\", whereas \"derivation\" instead refers to the process of arriving at a result.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-14T06:36:55","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hey Ryan,\u003cbr>sorry for replying so late and thank you very much for your corrections. Since I'm no english native, corrections like this are essential for me. So thanks again and I fixed the words.\u003c/p>","id":"3889199235","createdAt":"2018-05-07T06:36:55","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3842539330,"isApproved":true,"isFlagged":false,"raw_message":"Hey Ryan,\nsorry for replying so late and thank you very much for your corrections. Since I'm no english native, corrections like this are essential for me. So thanks again and I fixed the words.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-03-26T05:01:10","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Wow this is such amazing post! Thank you so much!\u003c/p>","id":"3812959741","createdAt":"2018-03-19T05:01:10","author":{"username":"jae_duk_seo","about":"Hello! Currently a fourth year comp sci student","name":"Jae Duk Seo","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-01-12T14:21:42","profileUrl":"https://disqus.com/by/jae_duk_seo/","url":"https://jaedukseo.me/","location":"Toronto, ON, Canada","isPrivate":false,"signedUrl":"https://disq.us/?url=https%3A%2F%2Fjaedukseo.me%2F&key=keRUg22y-Ty-EkyxWD2LbQ","isPrimary":true,"isAnonymous":false,"id":"276840001","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar32.jpg?1524419412"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar92.jpg?1524419412","large":{"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar92.jpg?1524419412"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Wow this is such amazing post! Thank you so much!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-03-03T08:09:13","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Amazing stuff man, I was trying to understand computational graphs for RELU and your post made everything so easy to follow.\u003cbr>Loved the clarity of explanation.\u003c/p>","id":"3773765640","createdAt":"2018-02-24T08:09:13","author":{"username":"vibhujawa","about":"","name":"Vibhu Jawa","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-07-04T09:59:58","profileUrl":"https://disqus.com/by/vibhujawa/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"113115540","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/vibhujawa.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/vibhujawa.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/vibhujawa.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Amazing stuff man, I was trying to understand computational graphs for RELU and your post made everything so easy to follow.\nLoved the clarity of explanation.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-03-04T18:05:25","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hey man, good to hear that my article could help you understanding computational graphs in general and that you were able to port it to a different operation! Good work\u003c/p>","id":"3775664434","createdAt":"2018-02-25T18:05:25","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3773765640,"isApproved":true,"isFlagged":false,"raw_message":"Hey man, good to hear that my article could help you understanding computational graphs in general and that you were able to port it to a different operation! Good work","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-02-15T19:05:06","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Amazing post, thank you for posting!\u003c/p>","id":"3749093115","createdAt":"2018-02-08T19:05:06","author":{"username":"jae_duk_seo","about":"Hello! Currently a fourth year comp sci student","name":"Jae Duk Seo","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-01-12T14:21:42","profileUrl":"https://disqus.com/by/jae_duk_seo/","url":"https://jaedukseo.me/","location":"Toronto, ON, Canada","isPrivate":false,"signedUrl":"https://disq.us/?url=https%3A%2F%2Fjaedukseo.me%2F&key=keRUg22y-Ty-EkyxWD2LbQ","isPrimary":true,"isAnonymous":false,"id":"276840001","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar32.jpg?1524419412"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar92.jpg?1524419412","large":{"permalink":"https://disqus.com/api/users/avatars/jae_duk_seo.jpg","cache":"https://c.disquscdn.com/uploads/users/27684/1/avatar92.jpg?1524419412"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Amazing post, thank you for posting!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-02-09T14:05:21","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>What a lovely post \u315c\u315c\u003c/p>","id":"3738782013","createdAt":"2018-02-02T14:05:21","author":{"username":"hyeungshikjung","about":"","name":"Hyeungshik Jung","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2013-06-07T00:26:12","profileUrl":"https://disqus.com/by/hyeungshikjung/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"55165620","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/hyeungshikjung.jpg","cache":"https://c.disquscdn.com/uploads/users/5516/5620/avatar32.jpg?1525912589"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/hyeungshikjung.jpg","cache":"https://c.disquscdn.com/uploads/users/5516/5620/avatar92.jpg?1525912589","large":{"permalink":"https://disqus.com/api/users/avatars/hyeungshikjung.jpg","cache":"https://c.disquscdn.com/uploads/users/5516/5620/avatar92.jpg?1525912589"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"What a lovely post \u315c\u315c","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-01-06T02:15:06","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>I have a problem with \"give me back my raw inputs\" or the un-doing part. The network can easily learn some parameters for a shift-and-scale transform, but could this really undo batchnorm? The normalization is done per-batch (with different means and variances each batch). The learned shift-scale transform is not per-batch but global for the complete dataset.\u003c/p>","id":"3683913718","createdAt":"2017-12-30T02:15:06","author":{"username":"tapsi_789","about":"","name":"tapsi","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-12-30T01:54:32","profileUrl":"https://disqus.com/by/tapsi_789/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"275457335","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"I have a problem with \"give me back my raw inputs\" or the un-doing part. The network can easily learn some parameters for a shift-and-scale transform, but could this really undo batchnorm? The normalization is done per-batch (with different means and variances each batch). The learned shift-scale transform is not per-batch but global for the complete dataset.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2018-01-11T09:51:19","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>No you are right, I can't undo the batch normalization of each batch perfectly, since it learns the shift and scale parameters as representatives of the entire training data. I would say numerically it won't be the same, but I think the distribution of your data/the activations would be close to the original one, since the scale and shift parameters are learned from your entire training data.\u003c/p>","id":"3691076340","createdAt":"2018-01-04T09:51:19","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3683913718,"isApproved":true,"isFlagged":false,"raw_message":"No you are right, I can't undo the batch normalization of each batch perfectly, since it learns the shift and scale parameters as representatives of the entire training data. I would say numerically it won't be the same, but I think the distribution of your data/the activations would be close to the original one, since the scale and shift parameters are learned from your entire training data.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-01-12T15:10:25","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks. This goes down to batch size then. For a 100 samples batch, the variance and the difference from the global distribution can be very, very high. So the training is done with each neuron almost randomly pulled around (according the neighbour samples in the batch). This is reported to have a nice regularizing effect, but in my experiment also a big source of noise that damages training noticeably. I'm experimenting with using running stats instead of just the current batch.\u003c/p>","id":"3693075455","createdAt":"2018-01-05T15:10:25","author":{"username":"tapsi_789","about":"","name":"tapsi","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-12-30T01:54:32","profileUrl":"https://disqus.com/by/tapsi_789/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"275457335","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/tapsi_789.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":true,"isDeleted":false,"isDeletedByAuthor":false,"parent":3691076340,"isApproved":true,"isFlagged":false,"raw_message":"Thanks. This goes down to batch size then. For a 100 samples batch, the variance and the difference from the global distribution can be very, very high. So the training is done with each neuron almost randomly pulled around (according the neighbour samples in the batch). This is reported to have a nice regularizing effect, but in my experiment also a big source of noise that damages training noticeably. I'm experimenting with using running stats instead of just the current batch.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-12-29T22:20:23","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Awesome work, helped a lot, thank you buddy..\u003c/p>","id":"3675016359","createdAt":"2017-12-22T22:20:23","author":{"username":"kanhavishva","about":"","name":"kanhavishva","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-05-29T10:59:13","profileUrl":"https://disqus.com/by/kanhavishva/","url":"","location":"","isPrivate":true,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"159548160","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/kanhavishva.jpg","cache":"https://c.disquscdn.com/uploads/users/15954/8160/avatar32.jpg?1558716130"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/kanhavishva.jpg","cache":"https://c.disquscdn.com/uploads/users/15954/8160/avatar92.jpg?1558716130","large":{"permalink":"https://disqus.com/api/users/avatars/kanhavishva.jpg","cache":"https://c.disquscdn.com/uploads/users/15954/8160/avatar92.jpg?1558716130"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Awesome work, helped a lot, thank you buddy..","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-12-30T19:54:14","dislikes":0,"numReports":0,"likes":1,"message":"\u003cp>Thanks a lot. I'm still astonished how many people actually read this artical. Glad to see it could help you too\u003c/p>","id":"3676081514","createdAt":"2017-12-23T19:54:14","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3675016359,"isApproved":true,"isFlagged":false,"raw_message":"Thanks a lot. I'm still astonished how many people actually read this artical. Glad to see it could help you too","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-12-01T18:11:49","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Why are we not averaging the sum of gradients of training examples in a mini-batch by the size of the mini-batch??\u003c/p>","id":"3631354318","createdAt":"2017-11-24T18:11:49","author":{"username":"rakhilimmidisetti","about":"","name":"Rakhil Immidisetti","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-11-24T18:11:33","profileUrl":"https://disqus.com/by/rakhilimmidisetti/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"272151587","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/rakhilimmidisetti.jpg","cache":"https://c.disquscdn.com/uploads/users/27215/1587/avatar32.jpg?1511547112"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/rakhilimmidisetti.jpg","cache":"https://c.disquscdn.com/uploads/users/27215/1587/avatar92.jpg?1511547112","large":{"permalink":"https://disqus.com/api/users/avatars/rakhilimmidisetti.jpg","cache":"https://c.disquscdn.com/uploads/users/27215/1587/avatar92.jpg?1511547112"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Why are we not averaging the sum of gradients of training examples in a mini-batch by the size of the mini-batch??","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-11-29T06:14:50","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Cool, thanks.\u003c/p>","id":"3627668880","createdAt":"2017-11-22T06:14:50","author":{"username":"zhuzii","about":"","name":"zhuzii","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2016-02-21T05:30:34","profileUrl":"https://disqus.com/by/zhuzii/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"197538510","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/zhuzii.jpg","cache":"https://c.disquscdn.com/uploads/users/19753/8510/avatar32.jpg?1511331292"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/zhuzii.jpg","cache":"https://c.disquscdn.com/uploads/users/19753/8510/avatar92.jpg?1511331292","large":{"permalink":"https://disqus.com/api/users/avatars/zhuzii.jpg","cache":"https://c.disquscdn.com/uploads/users/19753/8510/avatar92.jpg?1511331292"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Cool, thanks.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-10-23T21:19:38","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Nice work on this derivation! I was able to implement a JavaScript implementation of batch norm backpropagation.\u003c/p>\u003cp>The description is at \u003ca href=\"http://disq.us/url?url=http%3A%2F%2FmiabellaAI.net%2F%3AmZg9hp0mP9jfL2lQ_spC2Vavfxs&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"http://miabellaAI.net/\">http://miabellaAI.net/\u003c/a> and the web app is here: \u003ca href=\"http://disq.us/url?url=http%3A%2F%2Fann.miabellaAI.net%2F%3AXkKD8PC7tnMuxD7JHLis1E4SN3Q&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"http://ann.miabellaAI.net/\">http://ann.miabellaAI.net/\u003c/a>\u003c/p>\u003cp>Here are some tips for those who took Andrew Ng\u2019s deep learning course:\u003c/p>\u003cp>1. Andrew Ng and his team offer a backpropagation chain based on D-by-N matrices, which is the transpose of what is presented here. This needs to be taken into account at every step.\u003c/p>\u003cp>2. The Python code I\u2019ve seen at various places makes liberal use of broadcasting, which is when Python handles matrix-by-vector operations automatically. Depending on your platform, you might need to code this yourself, always keeping in mind the dimensions from point #1 above.\u003c/p>\u003cp>3. Gradient checking can be a pain to write, but it\u2019s the main way to make sure everything is hooked up correctly.\u003c/p>","id":"3570397972","createdAt":"2017-10-16T21:19:38","author":{"username":"disqus_nZ5GrhxFcX","about":"","name":"Writer","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-06-28T00:58:53","profileUrl":"https://disqus.com/by/disqus_nZ5GrhxFcX/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"112372310","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_nZ5GrhxFcX.jpg","cache":"https://c.disquscdn.com/uploads/users/11237/2310/avatar32.jpg?1403917655"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/disqus_nZ5GrhxFcX.jpg","cache":"https://c.disquscdn.com/uploads/users/11237/2310/avatar92.jpg?1403917655","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_nZ5GrhxFcX.jpg","cache":"https://c.disquscdn.com/uploads/users/11237/2310/avatar92.jpg?1403917655"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Nice work on this derivation! I was able to implement a JavaScript implementation of batch norm backpropagation.\n\nThe description is at http://miabellaAI.net/ and the web app is here: http://ann.miabellaAI.net/\n\nHere are some tips for those who took Andrew Ng\u2019s deep learning course:\n\n1. Andrew Ng and his team offer a backpropagation chain based on D-by-N matrices, which is the transpose of what is presented here. This needs to be taken into account at every step.\n\n2. The Python code I\u2019ve seen at various places makes liberal use of broadcasting, which is when Python handles matrix-by-vector operations automatically. Depending on your platform, you might need to code this yourself, always keeping in mind the dimensions from point #1 above.\n\n3. Gradient checking can be a pain to write, but it\u2019s the main way to make sure everything is hooked up correctly.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2017-09-30T11:23:13","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Dear, thank you so very much for sharing this. It did saved my life. I have one stupid question hope you dont mind. I tried implementing your way of backward prop and compared it with other ways I found on internet like this one: \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fkevinzakka.github.io%2F2016%2F09%2F14%2Fbatch_normalization%2F%3AtMo4ZSbuJOQUKdX33UaO9WftIFM&amp;cuid=4023296\" rel=\"nofollow noopener\" title=\"https://kevinzakka.github.io/2016/09/14/batch_normalization/\">https://kevinzakka.github.i...\u003c/a> and found that different methods might result in slightly different gradient outcomes. I was wondering if there is anyway I could test if my implementation is correct? like test cases of some sort? sorry for asking this cause I really had a hard time convincing myself my implementation is ok.  Thank you so very much.\u003c/p>","id":"3531385013","createdAt":"2017-09-23T11:23:13","author":{"username":"samuelpun","about":"Day Dreamer","name":"Samuel Pun","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-07-14T04:34:17","profileUrl":"https://disqus.com/by/samuelpun/","url":"","location":"Hong Kong","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"165366918","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar32.jpg?1508756167"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167","large":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Dear, thank you so very much for sharing this. It did saved my life. I have one stupid question hope you dont mind. I tried implementing your way of backward prop and compared it with other ways I found on internet like this one: https://kevinzakka.github.io/2016/09/14/batch_normalization/ and found that different methods might result in slightly different gradient outcomes. I was wondering if there is anyway I could test if my implementation is correct? like test cases of some sort? sorry for asking this cause I really had a hard time convincing myself my implementation is ok.  Thank you so very much.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2017-09-30T15:28:56","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Hey Samuel,\u003cbr>there are no stupid questions, so let me try to help you. There are definitely ways to test correct implementations. \u003cbr>E.g. if you have made the Machine Learning course by Andrew Ng at Cousera you might remember what I'm talking about. It's a long time ago since I made this course and studied the batch norm thing, so I don't remember exactly how it was done. If I remember correctly you have to calculate the gradient at some point explicitly by calculating \u003cbr>f'(x) = (f(x + delta_x) - f(x)) / delta_x\u003cbr>and then compare your gradients with this result, where delta_x is usually a pretty small value. This could be a direction to investigate for you, or have a look at the videos and code of Andrew Ng's course.\u003c/p>","id":"3531630005","createdAt":"2017-09-23T15:28:56","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3531385013,"isApproved":true,"isFlagged":false,"raw_message":"Hey Samuel,\nthere are no stupid questions, so let me try to help you. There are definitely ways to test correct implementations. \nE.g. if you have made the Machine Learning course by Andrew Ng at Cousera you might remember what I'm talking about. It's a long time ago since I made this course and studied the batch norm thing, so I don't remember exactly how it was done. If I remember correctly you have to calculate the gradient at some point explicitly by calculating \nf'(x) = (f(x + delta_x) - f(x)) / delta_x\nand then compare your gradients with this result, where delta_x is usually a pretty small value. This could be a direction to investigate for you, or have a look at the videos and code of Andrew Ng's course.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-09-30T17:30:57","dislikes":0,"numReports":0,"likes":2,"message":"\u003cp>Thanks for the reminder! I think that is gradient check. I shall give it a try! thank you so much!\u003c/p>","id":"3531808372","createdAt":"2017-09-23T17:30:57","author":{"username":"samuelpun","about":"Day Dreamer","name":"Samuel Pun","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-07-14T04:34:17","profileUrl":"https://disqus.com/by/samuelpun/","url":"","location":"Hong Kong","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"165366918","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar32.jpg?1508756167"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167","large":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167"}}},"media":[],"isSpam":false,"hasMore":true,"isDeleted":false,"isDeletedByAuthor":false,"parent":3531630005,"isApproved":true,"isFlagged":false,"raw_message":"Thanks for the reminder! I think that is gradient check. I shall give it a try! thank you so much!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":2,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-09-27T08:21:13","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Great blog post. Thank you!!!\u003c/p>","id":"3525944498","createdAt":"2017-09-20T08:21:13","author":{"username":"roeibahumi","about":"","name":"ROEI Bahumi","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2017-09-20T08:20:30","profileUrl":"https://disqus.com/by/roeibahumi/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"265730883","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/roeibahumi.jpg","cache":"https://c.disquscdn.com/uploads/users/26573/883/avatar32.jpg?1546865479"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/roeibahumi.jpg","cache":"https://c.disquscdn.com/uploads/users/26573/883/avatar92.jpg?1546865479","large":{"permalink":"https://disqus.com/api/users/avatars/roeibahumi.jpg","cache":"https://c.disquscdn.com/uploads/users/26573/883/avatar92.jpg?1546865479"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Great blog post. Thank you!!!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-09-27T08:28:31","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thanks!\u003c/p>","id":"3525949089","createdAt":"2017-09-20T08:28:31","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3525944498,"isApproved":true,"isFlagged":false,"raw_message":"Thanks!","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-09-25T19:02:48","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Also, why do we use squre root of variance? Why don't we divide by std+epsilon ?\u003cbr>numpy even has std in it...\u003c/p>","id":"3523460449","createdAt":"2017-09-18T19:02:48","author":{"username":"disqus_TaZmcVBYfi","about":"","name":"\u0415\u0432\u0433\u0435\u043d\u0438\u0439 \u0410\u0440\u0442\u0435\u043c\u0435\u043d\u043a\u043e","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-12-13T20:36:38","profileUrl":"https://disqus.com/by/disqus_TaZmcVBYfi/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"135568488","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_TaZmcVBYfi.jpg","cache":"https://c.disquscdn.com/uploads/users/13556/8488/avatar32.jpg?1418503238"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/disqus_TaZmcVBYfi.jpg","cache":"https://c.disquscdn.com/uploads/users/13556/8488/avatar92.jpg?1418503238","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_TaZmcVBYfi.jpg","cache":"https://c.disquscdn.com/uploads/users/13556/8488/avatar92.jpg?1418503238"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Also, why do we use squre root of variance? Why don't we divide by std+epsilon ?\nnumpy even has std in it...","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2017-09-26T13:03:06","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>I guess I don't get your question. If you compare the formulas from the first image with the graph you should see that it's basically the same. Also this is not about what numpy has or has not, it's about understanding of the gradient flow and how you can disassemble any algorithm into small pieces you can differentiate and chain together by the chain rule.\u003c/p>","id":"3524528765","createdAt":"2017-09-19T13:03:06","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3523460449,"isApproved":true,"isFlagged":false,"raw_message":"I guess I don't get your question. If you compare the formulas from the first image with the graph you should see that it's basically the same. Also this is not about what numpy has or has not, it's about understanding of the gradient flow and how you can disassemble any algorithm into small pieces you can differentiate and chain together by the chain rule.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-09-25T18:59:03","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>I don't understand dimensions in the first computational graph.\u003cbr>Do you have training examples in column-vectors or row-vectors?\u003cbr>Usually we have notation Rows x Columns for matrices:\u003cbr>Does D (number of columns) here correspond to m = training examples in mini-batch?\u003cbr>and N (number of rows) here correspond to n = activations in layer ? \u003cbr>\u003ca href=\"https://uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png\" rel=\"nofollow noopener\" title=\"https://uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png\">https://uploads.disquscdn.c...\u003c/a>\u003cbr>And why do we summing over N? Don't we need to sum over training examples , like this:\u003cbr>\u003ca href=\"https://uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png\" rel=\"nofollow noopener\" title=\"https://uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png\">https://uploads.disquscdn.c...\u003c/a>\u003c/p>","id":"3523454439","createdAt":"2017-09-18T18:59:03","author":{"username":"disqus_TaZmcVBYfi","about":"","name":"\u0415\u0432\u0433\u0435\u043d\u0438\u0439 \u0410\u0440\u0442\u0435\u043c\u0435\u043d\u043a\u043e","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-12-13T20:36:38","profileUrl":"https://disqus.com/by/disqus_TaZmcVBYfi/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"135568488","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_TaZmcVBYfi.jpg","cache":"https://c.disquscdn.com/uploads/users/13556/8488/avatar32.jpg?1418503238"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/disqus_TaZmcVBYfi.jpg","cache":"https://c.disquscdn.com/uploads/users/13556/8488/avatar92.jpg?1418503238","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_TaZmcVBYfi.jpg","cache":"https://c.disquscdn.com/uploads/users/13556/8488/avatar92.jpg?1418503238"}}},"media":[{"providerName":"Disquscdn","resolvedUrl":"https://uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png","thumbnailUrl":"//uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png","htmlHeight":null,"id":31185733,"thumbnailWidth":1116,"title":"","htmlWidth":null,"mediaType":"2","html":"","location":"https://uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png","type":"5","metadata":{"create_method":"preview","thumbnail":"//uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png"},"urlRedirect":"https://uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png","description":"","post":"3523454439","thumbnailURL":"//uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png","thread":"4617794568","forum":"kratzertblog","url":"https://uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png","resolvedUrlRedirect":"https://uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png","thumbnailHeight":441},{"providerName":"Disquscdn","resolvedUrl":"https://uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png","thumbnailUrl":"//uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png","htmlHeight":null,"id":31187511,"thumbnailWidth":607,"title":"","htmlWidth":null,"mediaType":"2","html":"","location":"https://uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png","type":"5","metadata":{"create_method":"preview","thumbnail":"//uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png"},"urlRedirect":"https://uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png","description":"","post":"3523454439","thumbnailURL":"//uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png","thread":"4617794568","forum":"kratzertblog","url":"https://uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png","resolvedUrlRedirect":"https://uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png","thumbnailHeight":557}],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"I don't understand dimensions in the first computational graph.\nDo you have training examples in column-vectors or row-vectors?\nUsually we have notation Rows x Columns for matrices:\nDoes D (number of columns) here correspond to m = training examples in mini-batch?\nand N (number of rows) here correspond to n = activations in layer ? \nhttps://uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png\nAnd why do we summing over N? Don't we need to sum over training examples , like this:\nhttps://uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2017-09-26T12:59:27","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>The matrix dimensions are row x columns as usual, so N is the number of training examples in the mini batch and D is the number of features. So it makes perfectly sense to sum over the N dimension, because batch normalization happens feature wise.\u003c/p>","id":"3524524166","createdAt":"2017-09-19T12:59:27","author":{"username":"fkratzert","about":"","name":"fkratzert","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2016-02-12T11:00:38","profileUrl":"https://disqus.com/by/fkratzert/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"196251915","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar32.png"},"isCustom":false,"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/fkratzert.jpg","cache":"//a.disquscdn.com/1561077851/images/noavatar92.png"}}},"media":[],"isSpam":false,"hasMore":true,"isDeleted":false,"isDeletedByAuthor":false,"parent":3523454439,"isApproved":true,"isFlagged":false,"raw_message":"The matrix dimensions are row x columns as usual, so N is the number of training examples in the mini batch and D is the number of features. So it makes perfectly sense to sum over the N dimension, because batch normalization happens feature wise.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2017-09-26T16:42:38","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>we have this as an input:\u003cbr>\u003ca href=\"https://uploads.disquscdn.com/images/81ee548150ece92fde048d1fe4086b9591cb0e44dbfe3802b7e455a89b24f490.png\" rel=\"nofollow noopener\" title=\"https://uploads.disquscdn.com/images/81ee548150ece92fde048d1fe4086b9591cb0e44dbfe3802b7e455a89b24f490.png\">https://uploads.disquscdn.c...\u003c/a>\u003c/p>","id":"3524853026","createdAt":"2017-09-19T16:42:38","author":{"username":"disqus_TaZmcVBYfi","about":"","name":"\u0415\u0432\u0433\u0435\u043d\u0438\u0439 \u0410\u0440\u0442\u0435\u043c\u0435\u043d\u043a\u043e","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-12-13T20:36:38","profileUrl":"https://disqus.com/by/disqus_TaZmcVBYfi/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"135568488","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_TaZmcVBYfi.jpg","cache":"https://c.disquscdn.com/uploads/users/13556/8488/avatar32.jpg?1418503238"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/disqus_TaZmcVBYfi.jpg","cache":"https://c.disquscdn.com/uploads/users/13556/8488/avatar92.jpg?1418503238","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_TaZmcVBYfi.jpg","cache":"https://c.disquscdn.com/uploads/users/13556/8488/avatar92.jpg?1418503238"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3524524166,"isApproved":true,"isFlagged":false,"raw_message":"we have this as an input:\nhttps://uploads.disquscdn.com/images/81ee548150ece92fde048d1fe4086b9591cb0e44dbfe3802b7e455a89b24f490.png","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":2,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2017-09-21T20:50:30","dislikes":0,"numReports":0,"likes":0,"message":"\u003cp>Thank you very much for this article. I learn a lot from you.\u003c/p>","id":"3517824785","createdAt":"2017-09-14T20:50:30","author":{"username":"samuelpun","about":"Day Dreamer","name":"Samuel Pun","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-07-14T04:34:17","profileUrl":"https://disqus.com/by/samuelpun/","url":"","location":"Hong Kong","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"165366918","avatar":{"small":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar32.jpg?1508756167"},"isCustom":true,"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167","large":{"permalink":"https://disqus.com/api/users/avatars/samuelpun.jpg","cache":"https://c.disquscdn.com/uploads/users/16536/6918/avatar92.jpg?1508756167"}}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isFlagged":false,"raw_message":"Thank you very much for this article. I learn a lot from you.","isHighlighted":false,"canVote":false,"thread":"4617794568","forum":"kratzertblog","depth":0,"points":0,"moderationLabels":[],"isEdited":false,"sb":false}],"thread":{"feed":"https://kratzertblog.disqus.com/understanding_the_backward_pass_through_batch_normalization_layer/latest.rss","author":"196251915","dislikes":0,"likes":54,"message":"","ratingsEnabled":false,"isSpam":false,"isDeleted":false,"category":"4451835","clean_title":"Understanding the backward pass through Batch Normalization Layer","id":"4617794568","signedLink":"https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&key=BEZsXC82hY3NSduqNJmwWQ","createdAt":"2016-02-28T01:28:04","hasStreaming":false,"raw_message":"","isClosed":false,"link":"https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html","slug":"understanding_the_backward_pass_through_batch_normalization_layer","forum":"kratzertblog","identifiers":[],"posts":119,"moderators":[196251915],"validateAllPosts":false,"title":"Understanding the backward pass through Batch Normalization Layer","highlightedPost":null}},"order":"popular"}</script>


    <div id="fixed-content"></div>


    
        
<script type="text/json" id="disqus-urls">{
    "root":"//disqus.com",
    "next":"https://c.disquscdn.com/next/current/embed"
}</script>

        
        <script>!function () {
            var d = document;
            var toH = d.head.appendChild.bind(d.head);

            var v = window.location.href.match(/[#&?]version=([0-9a-f]{32})/);
            var src = 'https://c.disquscdn.com/next/embed/lounge.load';
            if (v)
                src += '.' + v[1];
            src += '.js';

            var s = d.createElement('script');
            s.crossOrigin = 'anonymous';
            s.id = 'bootstrap-script';
            s.setAttribute('data-app', 'lounge');
            s.src = src;
            toH(s);

            var m = d.createElement('meta');
            m.setAttribute('http-equiv', 'Content-Security-Policy');
            m.setAttribute('content', "script-src https:;");
            toH(m);
        }();</script>
    


<script src="./common.bundle.4f78a44956523083e2be1589c3559f2b.js.다운로드"></script><div id="layout" data-tracking-area="layout"><div id="onboard" data-tracking-area="onboard"><div></div></div><div id="reactions__container"></div><div id="ratings__container"></div><div id="highlighted-post" data-tracking-area="highlighted" class="highlighted-post" style="display: none;"></div><div id="global-alert"></div><div id="tos__container"></div><header id="main-nav" data-tracking-area="main-nav"><nav class="nav nav-primary"><ul><li class="nav-tab nav-tab--primary tab-conversation active" data-role="post-count"><a class="publisher-nav-color"><span class="comment-count">119 comments</span><span class="comment-count-placeholder">Comments</span></a></li><li class="nav-tab nav-tab--primary tab-community"><a href="https://disqus.com/home/forums/kratzertblog/" class="publisher-nav-color" data-action="community-sidebar" data-forum="kratzertblog" id="community-tab" name="kratzertblog"><span class="community-name"><strong>kratzertblog</strong></span><strong class="community-name-placeholder">Community</strong></a></li><li class="nav-tab nav-tab--primary dropdown user-menu" data-role="logout"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="dropdown-toggle" data-toggle="dropdown" role="menuitem" name="Login"><span class="dropdown-toggle-wrapper"><span>Login</span> </span> <span class="caret"></span></a><ul class="dropdown-menu"><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="auth:disqus">Disqus</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="auth:facebook">Facebook</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="auth:twitter">Twitter</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="auth:google">Google</a></li></ul></li><li class="nav-tab nav-tab--primary notification-menu unread" data-role="notification-menu"><a href="https://disqus.com/home/inbox/" class="notification-container" data-action="home" data-home-path="home/inbox/"><span class="notification-icon icon-comment" aria-hidden="true"></span><span class="notification-count" data-role="notification-count">1</span></a></li></ul></nav></header><section id="conversation" data-role="main" data-tracking-area="main"><div class="nav nav-secondary" data-tracking-area="secondary-nav"><ul><li id="recommend-button" class="nav-tab nav-tab--secondary recommend dropdown"><div class="thread-likes"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="recommend" title="Recommend this discussion" class="dropdown-toggle "><span class="label label-default"><span class="recommend-icon icon-heart-empty"></span> Recommend</span><span class="label label-recommended"><span class="recommend-icon icon-heart"></span> Recommended</span> <span class="label label-count">54</span></a><ul class="dropdown-menu dropdown-menu--coachmark"><li><div><h2 class="coachmark__heading">Discussion Recommended!</h2><p class="coachmark__description">Recommending means this is a discussion worth sharing. It gets shared to your followers' Disqus feeds, and gives the creator kudos!</p></div> <a href="https://disqus.com/home/?utm_source=disqus_embed&amp;utm_content=recommend_btn" class="btn btn-primary coachmark__button" target="_blank">Find More Discussions</a></li></ul></div></div></li><li id="thread-share-bar" class="nav-tab nav-tab--secondary share-bar hidden-sm"><div class="thread-share-bar-buttons"><span class="thread-share__button share-twitter" data-action="share:twitter" tabindex="0"><span class="icon-twitter"></span><span class="share-text">Tweet</span></span><span class="thread-share__button share-facebook" data-action="share:facebook" tabindex="0"><span class="icon-facebook"></span><span class="share-text">Share</span></span></div></li><li data-role="post-sort" class="nav-tab nav-tab--secondary dropdown sorting pull-right"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="dropdown-toggle" data-toggle="dropdown">Sort by Best<span class="caret"></span></a><ul class="dropdown-menu pull-right"><li class="selected"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="sort" data-sort="popular">Best<i aria-hidden="true" class="icon-checkmark"></i></a></li><li class=""><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="sort" data-sort="desc">Newest<i aria-hidden="true" class="icon-checkmark"></i></a></li><li class=""><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="sort" data-sort="asc">Oldest<i aria-hidden="true" class="icon-checkmark"></i></a></li></ul></li></ul></div><div id="posts"><div id="form" class="textarea-outer-wrapper--top-level"><form class="reply"><div class="postbox"><div role="alert"></div><div class="avatar"><span class="user"><img data-role="user-avatar" src="./noavatar92.7b2fde640943965cc88df0cdee365907.png" alt="Avatar"></span></div><div class="textarea-outer-wrapper"><div class="ratings-wrapper" data-role="ratings-container"></div><div class="textarea-wrapper" data-role="textarea" dir="auto"><div><span class="placeholder">Join the discussion…</span><div class="textarea" tabindex="0" role="textbox" aria-multiline="true" contenteditable="PLAINTEXT-ONLY" data-role="editable" aria-label="Join the discussion…" style="overflow: auto; overflow-wrap: break-word; max-height: 350px;"><p><br></p></div><div style="display: none;"><ul class="user-mention__list"><li class="header user-mention__header"><h5>in this conversation</h5></li></ul></div></div><div data-role="drag-drop-placeholder" class="media-drag-hover" style="display: none;"><div class="drag-text">⬇ Drag and drop your images here to upload them.</div></div><div class="media-preview empty" data-role="media-preview"><ul data-role="media-progress-list"></ul>
<ul data-role="media-rich-list"></ul>
<div class="media-expanded empty" data-role="media-preview-expanded">
<img src="data:image/gif;base64,R0lGODlhAQABAAAAACH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" data-role="media-preview-expanded-image" alt="Media preview placeholder">
</div>
</div><div class="edit-alert" role="postbox-alert"></div><div class="text-editor-container"><div class="post-actions"><div class="wysiwyg"><div class="gif-picker"><div class="wysiwyg__item" data-role="gif-picker-toggle" title="GIF">GIF</div><div class="hidden gif-picker__popout-container" data-role="gif-picker-popout-container"></div><img class="new-feature-badge-star" src="./star-badge.057756082fb42806b4b483567fc0399a.svg" title="New"></div><div class="media-uploader"><li class="wysiwyg__item" data-role="media-uploader"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" tabindex="-1" data-action="attach" class="attach" title="Upload Images">
<img alt="A" src="./attach.03c320b14aa9c071da30c904d0a0827f.svg">
</a>
<input type="file" data-role="media-upload" tabindex="-1" accept="image/*">
</li></div><div class="vertical-separator"></div><div data-action="text-editor-buttons"><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="b"><img alt="[object HTMLElement]" src="./bold.cb366e6a49396fb0e47a01df277563c8.svg" title="Bold"></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="i"><img alt="[object HTMLElement]" src="./italic.a6e1da4a89899ae5e87db9ded9f84d5b.svg" title="Italic"></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="u"><img alt="[object HTMLElement]" src="./underline.59f82f5f5bbed90fd72132ef98662fe3.svg" title="Underline"></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="s"><img alt="[object HTMLElement]" src="./strikethrough.ced68e63961c6bc0e072ce907906b252.svg" title="Strikethrough"></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="a"><img alt="L" src="./link.5ef9a39f22ce49f926e304567b9d611b.svg" title="Link"></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="spoiler"><img alt="S" src="./spoiler.eff5de8f72591c5ceeb4fa26a117c6d1.svg" title="Spoiler"></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="code"><img alt="C" src="./code.8f558a246aa4e9c41ef343f72f012f01.svg" title="Code"></div><div class="wysiwyg__item" data-action="text-editor-tag" data-tag="blockquote"><img alt="B" src="./blockquote.69435f6faa8c7a193456c46bcb7fb1ed.svg" title="Quote"></div></div></div></div></div></div></div><div data-role="login-form"><div><div><section class="auth-section logged-out__display"><div class="connect"><h6>Log in with</h6><ul data-role="login-menu" class="services login-buttons"><li class="auth-disqus"><button type="button" data-action="auth:disqus" title="Disqus" class="connect__button"><i class="icon-disqus"></i></button></li><li class="auth-facebook"><button type="button" data-action="auth:facebook" title="Facebook" class="connect__button"><i class="icon-facebook-circle"></i></button></li><li class="auth-twitter"><button type="button" data-action="auth:twitter" title="Twitter" class="connect__button"><i class="icon-twitter-circle"></i></button></li><li class="auth-google"><button type="button" data-action="auth:google" title="Google" class="connect__button"><i class="icon-google-plus-circle"></i></button></li></ul></div><div class="guest"><h6 class="guest-form-title"><span class="register-text"> or sign up with Disqus </span><span class="guest-text"> or pick a name </span></h6> <div class="help-tooltip__wrapper help-icon" tabindex="0"><div id="rules" class="tooltip show help-tooltip"><h3 class="help-tooltip__heading">Disqus is a discussion network</h3><ul class="help-tooltip__list"><li><span>Disqus never moderates or censors. The rules on this community are its own.</span></li><li><span>Don't be a jerk or do anything illegal. Everything is easier that way.</span></li></ul><p class="clearfix"><a href="https://docs.disqus.com/kb/terms-and-policies/" class="btn btn-small help-tooltip__button" target="_blank">Read full terms and conditions</a></p></div></div><p class="input-wrapper"><input dir="auto" type="text" placeholder="Name" name="display_name" id="view108_display_name" maxlength="30" class="input--text" aria-label="name"></p><div class="guest-details " data-role="guest-details"><p class="input-wrapper"><input dir="auto" type="email" placeholder="Email" name="email" id="view108_email" class="input--text" aria-label="email"></p><p class="input-wrapper"><input dir="auto" disabled="" type="text" class="register-text input--text" placeholder="Password" name="password" aria-label="password" id="view108_password"></p><p><label><span>Please access our <a href="https://help.disqus.com/customer/portal/articles/466259-privacy-policy" target="_blank" rel="noopener noreferrer">Privacy Policy</a> to learn what personal data Disqus collects and your choices about how it is used.  All users of our service are also subject to our <a href="https://help.disqus.com/customer/portal/articles/466260-terms-of-service" target="_blank" rel="noopener noreferrer">Terms of Service</a>.</span></label></p><input type="checkbox" name="author-guest" style="display: none;"><div class="g-recaptcha" data-role="grecaptcha-container"></div><div class="proceed" data-role="submit-btn-container"><button type="submit" class="proceed__button btn submit" aria-label="Next"><span class="icon-proceed"></span><div class="spinner"></div></button></div></div></div></section></div></div></div></div></form></div><button class="alert alert--realtime" data-role="realtime-notification" style="display: none;"></button><div id="email-signup"></div><div id="no-posts" style="display: none;"></div><ul id="post-list" class="post-list"><li class="post" id="post-3638799277"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3638799277"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_cxY7uKUvpl/" data-action="profile" data-username="disqus_cxY7uKUvpl" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="272639134" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_cxY7uKUvpl/" data-action="profile" data-username="disqus_cxY7uKUvpl" target="_blank" rel="noopener noreferrer">aboul</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3638799277" data-role="relative-time" class="time-ago" title="Thursday, November 30, 2017 6:19 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks ! the flowchart help me so much to implement in R ;-)<br><span></span><div class="media-container media-mode-deferred">
<a class="media-button media-button-expand publisher-color publisher-border-color" href="https://uploads.disquscdn.com/images/1e337ec58baa9748bbf31a3841a11ab3bc3c9e26a213ad186a7a14869dad30ce.jpg" rel="nofollow" target="_blank" data-action="expand" title="">
<i class="icon-images publisher-background-color"></i>
View
</a>
<a class="media-button media-button-contract publisher-color publisher-border-color" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" target="_blank" data-action="contract">
<i class="icon-cancel publisher-background-color"></i> Hide
</a>
<div class="media-content-loader" data-role="content-loader"></div>
<div data-role="content-placeholder" class="media-content-placeholder media-Disquscdn " style="height: 513px;"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="media-force-load" data-action="force-load"><i class="icon-images"></i></a>
</div>
</div></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-3" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">3</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3638799277"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3639479740"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3639479740"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3638799277" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> aboul</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3639479740" data-role="relative-time" class="time-ago" title="Thursday, November 30, 2017 5:58 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Looks like my sketches, when I first sat down to derive the graph for my self. Good work!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3639479740"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3678638435"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3678638435"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/patricklangechuanliu/" data-action="profile" data-username="patricklangechuanliu" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="275100268" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/patricklangechuanliu/" data-action="profile" data-username="patricklangechuanliu" target="_blank" rel="noopener noreferrer">Patrick Langechuan LIU</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3639479740" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3678638435" data-role="relative-time" class="time-ago" title="Tuesday, December 26, 2017 4:18 PM">2 years ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hi Frederik,</p><p>Thanks for the great explanation in this post, but I had some trouble understanding the backprop in step 9, specifically why an np.sum() is used rather than np.mean() to calculate dbeta. My intuition is, during the addition step with numpy broadcasting to every row, any error in beta is duplicated n_row times, and thus dbeta should be the average across rows of dout.</p><p>From what I can see in aboul's handwriting in the flow chart above, he used average rather than sum. I was wondering if you could shed some light onto this issue. Thanks!</p><p>Patrick</p><p>==========<br>Edit: Never mind. I found the explanation on the link you gave at the end of the post <a href="http://disq.us/url?url=http%3A%2F%2Fcthorey.github.io%2Fbackpropagation%2F%3Ar33DVWl8q8rDRF9zRhyrL5527fg&amp;cuid=4023296" rel="nofollow noopener" title="http://cthorey.github.io/backpropagation/">http://cthorey.github.io/ba...</a> in the section "We can therefore chain the gradient of the loss with respect to the input h_ij by the gradient of the loss with respect to ALL the outputs y_kl which reads ...". This is related to the notion of "total derivative" <a href="https://disq.us/url?url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FTotal_derivative%3AdO1EtjQujaspBuHilHbsqfsJopg&amp;cuid=4023296" rel="nofollow noopener" title="https://en.wikipedia.org/wiki/Total_derivative">https://en.wikipedia.org/wi...</a>.</p><p>In addition, I agree with John McIain's explanation above that a more appropriate explanation about why the broadcast items are added up but are not averaged is that the final cost is the summed across different rows,</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3678638435"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3678638435-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3639479740-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3638799277-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4355771341"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4355771341"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_OcxfWmpedA/" data-action="profile" data-username="disqus_OcxfWmpedA" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="43571292" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_OcxfWmpedA/" data-action="profile" data-username="disqus_OcxfWmpedA" target="_blank" rel="noopener noreferrer">Rahul Devanarayanan</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-4355771341" data-role="relative-time" class="time-ago" title="Wednesday, February 27, 2019 9:44 AM">5 months ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hi Frederik,</p><p>I'm having trouble understanding your derivation for dBeta and dGamma. Why do you take the sum of the gradient over all the backprop inputs for that node? Shouldn't it be the average of all the backprop inputs for that node?</p><p>To my knowledge, that's how dTheta terms are calculated in other layers of the neural network as well.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-2" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">2</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4355771341"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4355771341-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3677924058"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3677924058"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_lDJKDl7VxM/" data-action="profile" data-username="disqus_lDJKDl7VxM" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="275048753" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_lDJKDl7VxM/" data-action="profile" data-username="disqus_lDJKDl7VxM" target="_blank" rel="noopener noreferrer">JohnMclain</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3677924058" data-role="relative-time" class="time-ago" title="Tuesday, December 26, 2017 12:28 AM">2 years ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>hi，<br>I believe there is something wrong with your explanation about why the broadcast terms are added up. For example beta are summed in columns.</p><p>"And because the summation of beta during the forward pass is a row-wise summation, during the backward pass we need to sum up the gradient over all of its columns (take a look at the dimensions)."</p><p>I think the reason is that in the end loss different training examples are summed for the final loss evaluation. Not the reason you give.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-2" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">2</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3677924058"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3708575158"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3708575158"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3677924058" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> JohnMclain</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3708575158" data-role="relative-time" class="time-ago" title="Monday, January 15, 2018 5:40 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Sorry John for replying so late, I somehow haven't seen your post but was now directed through another comment to your comment again. And well I think you are totally right (I adapted the passages in step 9 and 8 of the backprop, you might take a look and give me your opinion on the updated version).</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3708575158"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3879045238"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3879045238"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/harveyqiu/" data-action="profile" data-username="harveyqiu" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="286645466" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/harveyqiu/" data-action="profile" data-username="harveyqiu" target="_blank" rel="noopener noreferrer">Harvey Qiu</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3708575158" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3879045238" data-role="relative-time" class="time-ago" title="Monday, April 30, 2018 6:04 PM">a year ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hi, I have been struggling with getting this part for a while. I ended up writing out the full expression of dL/d\beta (or any broadcasting part) to arrive at the same result (quite some work for one step, phew).</p><p>I think the reason for adding up rows is the multivariate-calculus rule which says "gradients flow into different branches of the graph should be summed". Even if the loss function at the end is not a sum, we still should sum the gradient from different branches that flow into one variable.</p><p>As a concrete example, in step 9, a term of dL/d\beta should be:<br> <span></span><div class="media-container media-mode-deferred">
<a class="media-button media-button-expand publisher-color publisher-border-color" href="https://uploads.disquscdn.com/images/1cab7ea5bf304e2313db0d962b0dc62e603f1c434c305b55fae7ac63e3adcc7a.gif" rel="nofollow" target="_blank" data-action="expand" title="">
<i class="icon-images publisher-background-color"></i>
View
</a>
<a class="media-button media-button-contract publisher-color publisher-border-color" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" target="_blank" data-action="contract">
<i class="icon-cancel publisher-background-color"></i> Hide
</a>
<div class="media-content-loader" data-role="content-loader"></div>
<div data-role="content-placeholder" class="media-content-placeholder media-Disquscdn " style="height: 49px;"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="media-force-load" data-action="force-load"><i class="icon-images"></i></a>
</div>
</div> <br>where z_tilde is the output in your notation. Obviously only the i-th column of the output is involved in the forward path so this derivative is the sum of the i-th column of dout.</p><p>However, I do have a question. I found myself in need of doing this full-out derivation basically every time which is quite non-trivial. What's your thinking track that leads to the expression without all the element-wise writting-out?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3879045238"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper "><a class="show-children" id="post-3879045238-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3708575158-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3677924058-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3772850279"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3772850279"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_GJH1JRIogE/" data-action="profile" data-username="disqus_GJH1JRIogE" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="280979018" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_GJH1JRIogE/" data-action="profile" data-username="disqus_GJH1JRIogE" target="_blank" rel="noopener noreferrer">Jorge</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3772850279" data-role="relative-time" class="time-ago" title="Saturday, February 24, 2018 3:41 AM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks fkratzert for your effort !!<br>I verified your code passes Andrew NG's Gradient Checking.<br>Cheers!!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3772850279"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3772882856"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3772882856"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3772850279" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Jorge</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3772882856" data-role="relative-time" class="time-ago" title="Saturday, February 24, 2018 4:00 AM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks man, good to hear ;)</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3772882856"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3772882856-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3772850279-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3579296064"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3579296064"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/anandsaha/" data-action="profile" data-username="anandsaha" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="7128646" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/anandsaha/" data-action="profile" data-username="anandsaha" target="_blank" rel="noopener noreferrer">Anand Saha</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3579296064" data-role="relative-time" class="time-ago" title="Monday, October 23, 2017 1:34 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thank you! You have a knack of simplifying complexity :-)</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3579296064"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3580214271"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3580214271"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3579296064" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Anand Saha</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3580214271" data-role="relative-time" class="time-ago" title="Monday, October 23, 2017 4:16 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks, this is really what I tried. I had a hard time to work myself through this topic, back when I followed the cs231n online and set many hours in front of a paper an tried to derive myself everything on my own. At the end I broke everything down to this really simple steps.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3580214271"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4541606339"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4541606339"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_UXlWQroB4n/" data-action="profile" data-username="disqus_UXlWQroB4n" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="122867415" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_UXlWQroB4n/" data-action="profile" data-username="disqus_UXlWQroB4n" target="_blank" rel="noopener noreferrer">禹邵 賴</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3580214271" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-4541606339" data-role="relative-time" class="time-ago" title="Tuesday, July 16, 2019 9:36 PM">a day ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>But still wrong. Still not give a precise explanation about the sigma of beta term.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4541606339"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4541606339-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3580214271-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3579296064-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3561820127"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3561820127"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/majicji/" data-action="profile" data-username="majicji" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="267800662" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/majicji/" data-action="profile" data-username="majicji" target="_blank" rel="noopener noreferrer">Majic Ji</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3561820127" data-role="relative-time" class="time-ago" title="Wednesday, October 11, 2017 4:15 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks. I got the intuition of how to derive the derivative of  np.sum along one axis.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3561820127"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3578840984"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3578840984"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3561820127" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Majic Ji</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3578840984" data-role="relative-time" class="time-ago" title="Sunday, October 22, 2017 5:31 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Glad I could help you.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3578840984"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3578840984-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3561820127-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3268047638"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3268047638"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/jschaeff/" data-action="profile" data-username="jschaeff" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="249599616" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/jschaeff/" data-action="profile" data-username="jschaeff" target="_blank" rel="noopener noreferrer">jschaeff</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268047638" data-role="relative-time" class="time-ago" title="Saturday, April 22, 2017 11:49 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Really great post.  Once you have calculated the backward gradients, are they immediately subtracted from the gamma and beta terms to update them before forward propogating the next training batch?  And once all minibatches are completed, are these terms reset to 1 and 0 before the next epoch?  I am thinking of cases where each complete forward and backward training epoch is self-contained and independent from other epochs, returning only the updated weights with each pass.  I'm also wondering how to implement batch normalization for testing when the gamma and beta from training is unknown.<br>Thanks!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3268047638"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3268555117"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3268555117"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268047638" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> jschaeff</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268555117" data-role="relative-time" class="time-ago" title="Saturday, April 22, 2017 11:23 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Regarding your first questions: Yes you update all variables of the network immediately, when you have computed the gradients. What would be the purpose of passing another batch through the network without updating the knowledge gained from the last batch into the network? <br>And regarding your second question: No you don't reset gamma and beta after each epoch. It's the same as for all the other parameters, which you don't reset after each epoch. Remember that you are updating parameters with gradients and a learning rate, so you only take small steps to the direction of a minimum.</p><p>And for the last, I don't know if I understand you correctly. You want to apply BatchNorm to a already trained network, that you didn't trained with BatchNorm? What do you expect what will happen?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3268555117"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3268622523"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3268622523"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/jschaeff/" data-action="profile" data-username="jschaeff" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="249599616" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/jschaeff/" data-action="profile" data-username="jschaeff" target="_blank" rel="noopener noreferrer">jschaeff</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268555117" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3268622523" data-role="relative-time" class="time-ago" title="Sunday, April 23, 2017 12:22 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Yes, my network wasn't converging if updating gamma and beta, but I realize now, after your reply, that I was simply subtracting the dgamma and dbeta terms without considering the network learning rate.  Thank you!!!</p><p>For the last question, I was unclear.  Say you download the weights for a network, pretrained with BN.  Shouldn't the BN be included also at test time in inference mode, as a deterministic transform?  According to the Ioffe paper, this linear transform uses gamma and beta, along with the training population running means and variances.  My question pertains to when the training data is unavailable.  The training means and variances could possibly be approximated from a complete forward pass with the testing data, using the gamma and beta - but what to do when you don't have these terms?</p><p>Thanks again</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3268622523"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper "><a class="show-children" id="post-3268622523-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3268555117-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3268047638-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4439404565"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4439404565"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/dhawal_gupta/" data-action="profile" data-username="dhawal_gupta" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="331836313" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/dhawal_gupta/" data-action="profile" data-username="dhawal_gupta" target="_blank" rel="noopener noreferrer">Dhawal Gupta</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-4439404565" data-role="relative-time" class="time-ago" title="Saturday, April 27, 2019 5:01 AM">3 months ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Great blog, doing gods work, thank you for such a clear explanation.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4439404565"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4439404565-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4331155064"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4331155064"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/karthikeyanmg/" data-action="profile" data-username="karthikeyanmg" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="102110915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/karthikeyanmg/" data-action="profile" data-username="karthikeyanmg" target="_blank" rel="noopener noreferrer">Karthikeyan Mg</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-4331155064" data-role="relative-time" class="time-ago" title="Monday, February 11, 2019 3:47 AM">5 months ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Good one! <br>Is there a video version of this blog in cs231n? If so can you link the specific video?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4331155064"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4331176687"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="4331176687"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-4331155064" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Karthikeyan Mg</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-4331176687" data-role="relative-time" class="time-ago" title="Monday, February 11, 2019 4:05 AM">5 months ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>You mean if they explain it in class somewhere? No they don't, at least not when I did the class by watching the videos. There is one point where Andrej explains chain rule and computational graphs, but never the backpass through the batchnorm layer</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4331176687"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4331176687-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4331155064-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3895466348"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3895466348"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_dVXj63cxog/" data-action="profile" data-username="disqus_dVXj63cxog" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="238245013" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_dVXj63cxog/" data-action="profile" data-username="disqus_dVXj63cxog" target="_blank" rel="noopener noreferrer">Michael Green</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3895466348" data-role="relative-time" class="time-ago" title="Friday, May 11, 2018 3:09 PM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Nice writeup. :)</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3895466348"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3895523131"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3895523131"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3895466348" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Michael Green</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3895523131" data-role="relative-time" class="time-ago" title="Friday, May 11, 2018 4:52 PM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks :)</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3895523131"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3895523131-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3895466348-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3874235827"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3874235827"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/abhisheknadgeri/" data-action="profile" data-username="abhisheknadgeri" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="286384918" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/abhisheknadgeri/" data-action="profile" data-username="abhisheknadgeri" target="_blank" rel="noopener noreferrer">Abhishek Nadgeri</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3874235827" data-role="relative-time" class="time-ago" title="Friday, April 27, 2018 3:53 AM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for the post I loved it :D</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3874235827"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3889199453"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3889199453"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3874235827" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Abhishek Nadgeri</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3889199453" data-role="relative-time" class="time-ago" title="Monday, May 7, 2018 3:37 PM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>you are welcome. great that you liked it!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3889199453"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3889199453-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3874235827-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3842539330"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3842539330"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/ryanneph/" data-action="profile" data-username="ryanneph" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="284757841" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/ryanneph/" data-action="profile" data-username="ryanneph" target="_blank" rel="noopener noreferrer">Ryan Neph</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3842539330" data-role="relative-time" class="time-ago" title="Saturday, April 7, 2018 5:42 AM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for the nice explanation. A small terminological suggestion: when you discuss the derivative /gradient of a function, it is widely accepted to use the term "derivative", whereas "derivation" instead refers to the process of arriving at a result.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3842539330"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3889199235"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3889199235"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3842539330" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Ryan Neph</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3889199235" data-role="relative-time" class="time-ago" title="Monday, May 7, 2018 3:36 PM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hey Ryan,<br>sorry for replying so late and thank you very much for your corrections. Since I'm no english native, corrections like this are essential for me. So thanks again and I fixed the words.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3889199235"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3889199235-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3842539330-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3812959741"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3812959741"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/jae_duk_seo/" data-action="profile" data-username="jae_duk_seo" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="276840001" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/jae_duk_seo/" data-action="profile" data-username="jae_duk_seo" target="_blank" rel="noopener noreferrer">Jae Duk Seo</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3812959741" data-role="relative-time" class="time-ago" title="Monday, March 19, 2018 2:01 PM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Wow this is such amazing post! Thank you so much!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3812959741"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3812959741-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3773765640"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3773765640"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/vibhujawa/" data-action="profile" data-username="vibhujawa" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="113115540" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/vibhujawa/" data-action="profile" data-username="vibhujawa" target="_blank" rel="noopener noreferrer">Vibhu Jawa</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3773765640" data-role="relative-time" class="time-ago" title="Saturday, February 24, 2018 5:09 PM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Amazing stuff man, I was trying to understand computational graphs for RELU and your post made everything so easy to follow.<br>Loved the clarity of explanation.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3773765640"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3775664434"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3775664434"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3773765640" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Vibhu Jawa</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3775664434" data-role="relative-time" class="time-ago" title="Monday, February 26, 2018 3:05 AM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hey man, good to hear that my article could help you understanding computational graphs in general and that you were able to port it to a different operation! Good work</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3775664434"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3775664434-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3773765640-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3749093115"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3749093115"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/jae_duk_seo/" data-action="profile" data-username="jae_duk_seo" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="276840001" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/jae_duk_seo/" data-action="profile" data-username="jae_duk_seo" target="_blank" rel="noopener noreferrer">Jae Duk Seo</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3749093115" data-role="relative-time" class="time-ago" title="Friday, February 9, 2018 4:05 AM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Amazing post, thank you for posting!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3749093115"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3749093115-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3738782013"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3738782013"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/hyeungshikjung/" data-action="profile" data-username="hyeungshikjung" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="55165620" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/hyeungshikjung/" data-action="profile" data-username="hyeungshikjung" target="_blank" rel="noopener noreferrer">Hyeungshik Jung</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3738782013" data-role="relative-time" class="time-ago" title="Friday, February 2, 2018 11:05 PM">a year ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>What a lovely post ㅜㅜ</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3738782013"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3738782013-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3683913718"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3683913718"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/tapsi_789/" data-action="profile" data-username="tapsi_789" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="275457335" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/tapsi_789/" data-action="profile" data-username="tapsi_789" target="_blank" rel="noopener noreferrer">tapsi</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3683913718" data-role="relative-time" class="time-ago" title="Saturday, December 30, 2017 11:15 AM">2 years ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I have a problem with "give me back my raw inputs" or the un-doing part. The network can easily learn some parameters for a shift-and-scale transform, but could this really undo batchnorm? The normalization is done per-batch (with different means and variances each batch). The learned shift-scale transform is not per-batch but global for the complete dataset.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3683913718"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3691076340"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3691076340"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3683913718" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> tapsi</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3691076340" data-role="relative-time" class="time-ago" title="Thursday, January 4, 2018 6:51 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>No you are right, I can't undo the batch normalization of each batch perfectly, since it learns the shift and scale parameters as representatives of the entire training data. I would say numerically it won't be the same, but I think the distribution of your data/the activations would be close to the original one, since the scale and shift parameters are learned from your entire training data.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3691076340"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3693075455"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3693075455"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/tapsi_789/" data-action="profile" data-username="tapsi_789" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="275457335" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/tapsi_789/" data-action="profile" data-username="tapsi_789" target="_blank" rel="noopener noreferrer">tapsi</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3691076340" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3693075455" data-role="relative-time" class="time-ago" title="Saturday, January 6, 2018 12:10 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks. This goes down to batch size then. For a 100 samples batch, the variance and the difference from the global distribution can be very, very high. So the training is done with each neuron almost randomly pulled around (according the neighbour samples in the batch). This is reported to have a nice regularizing effect, but in my experiment also a big source of noise that damages training noticeably. I'm experimenting with using running stats instead of just the current batch.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3693075455"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper "><a class="show-children" id="post-3693075455-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3691076340-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3683913718-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3675016359"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3675016359"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/kanhavishva/" data-action="profile" data-username="kanhavishva" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="159548160" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/kanhavishva/" data-action="profile" data-username="kanhavishva" target="_blank" rel="noopener noreferrer">kanhavishva</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3675016359" data-role="relative-time" class="time-ago" title="Saturday, December 23, 2017 7:20 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Awesome work, helped a lot, thank you buddy..</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3675016359"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3676081514"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3676081514"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3675016359" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> kanhavishva</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3676081514" data-role="relative-time" class="time-ago" title="Sunday, December 24, 2017 4:54 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks a lot. I'm still astonished how many people actually read this artical. Glad to see it could help you too</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">1</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3676081514"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3676081514-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3675016359-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3631354318"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3631354318"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/rakhilimmidisetti/" data-action="profile" data-username="rakhilimmidisetti" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="272151587" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/rakhilimmidisetti/" data-action="profile" data-username="rakhilimmidisetti" target="_blank" rel="noopener noreferrer">Rakhil Immidisetti</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3631354318" data-role="relative-time" class="time-ago" title="Saturday, November 25, 2017 3:11 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Why are we not averaging the sum of gradients of training examples in a mini-batch by the size of the mini-batch??</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3631354318"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3631354318-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3627668880"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3627668880"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/zhuzii/" data-action="profile" data-username="zhuzii" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="197538510" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/zhuzii/" data-action="profile" data-username="zhuzii" target="_blank" rel="noopener noreferrer">zhuzii</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3627668880" data-role="relative-time" class="time-ago" title="Wednesday, November 22, 2017 3:14 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Cool, thanks.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3627668880"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3627668880-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3570397972"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3570397972"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_nZ5GrhxFcX/" data-action="profile" data-username="disqus_nZ5GrhxFcX" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="112372310" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_nZ5GrhxFcX/" data-action="profile" data-username="disqus_nZ5GrhxFcX" target="_blank" rel="noopener noreferrer">Writer</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3570397972" data-role="relative-time" class="time-ago" title="Tuesday, October 17, 2017 6:19 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Nice work on this derivation! I was able to implement a JavaScript implementation of batch norm backpropagation.</p><p>The description is at <a href="http://disq.us/url?url=http%3A%2F%2FmiabellaAI.net%2F%3AmZg9hp0mP9jfL2lQ_spC2Vavfxs&amp;cuid=4023296" rel="nofollow noopener" title="http://miabellaAI.net/">http://miabellaAI.net/</a> and the web app is here: <a href="http://disq.us/url?url=http%3A%2F%2Fann.miabellaAI.net%2F%3AXkKD8PC7tnMuxD7JHLis1E4SN3Q&amp;cuid=4023296" rel="nofollow noopener" title="http://ann.miabellaAI.net/">http://ann.miabellaAI.net/</a></p><p>Here are some tips for those who took Andrew Ng’s deep learning course:</p><p>1. Andrew Ng and his team offer a backpropagation chain based on D-by-N matrices, which is the transpose of what is presented here. This needs to be taken into account at every step.</p><p>2. The Python code I’ve seen at various places makes liberal use of broadcasting, which is when Python handles matrix-by-vector operations automatically. Depending on your platform, you might need to code this yourself, always keeping in mind the dimensions from point #1 above.</p><p>3. Gradient checking can be a pain to write, but it’s the main way to make sure everything is hooked up correctly.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3570397972"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3570397972-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3531385013"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3531385013"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="165366918" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer">Samuel Pun</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531385013" data-role="relative-time" class="time-ago" title="Saturday, September 23, 2017 8:23 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Dear, thank you so very much for sharing this. It did saved my life. I have one stupid question hope you dont mind. I tried implementing your way of backward prop and compared it with other ways I found on internet like this one: <a href="https://disq.us/url?url=https%3A%2F%2Fkevinzakka.github.io%2F2016%2F09%2F14%2Fbatch_normalization%2F%3AtMo4ZSbuJOQUKdX33UaO9WftIFM&amp;cuid=4023296" rel="nofollow noopener" title="https://kevinzakka.github.io/2016/09/14/batch_normalization/">https://kevinzakka.github.i...</a> and found that different methods might result in slightly different gradient outcomes. I was wondering if there is anyway I could test if my implementation is correct? like test cases of some sort? sorry for asking this cause I really had a hard time convincing myself my implementation is ok.  Thank you so very much.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3531385013"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3531630005"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3531630005"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531385013" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Samuel Pun</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531630005" data-role="relative-time" class="time-ago" title="Sunday, September 24, 2017 12:28 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hey Samuel,<br>there are no stupid questions, so let me try to help you. There are definitely ways to test correct implementations. <br>E.g. if you have made the Machine Learning course by Andrew Ng at Cousera you might remember what I'm talking about. It's a long time ago since I made this course and studied the batch norm thing, so I don't remember exactly how it was done. If I remember correctly you have to calculate the gradient at some point explicitly by calculating <br>f'(x) = (f(x + delta_x) - f(x)) / delta_x<br>and then compare your gradients with this result, where delta_x is usually a pretty small value. This could be a direction to investigate for you, or have a look at the videos and code of Andrew Ng's course.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3531630005"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3531808372"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3531808372"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="165366918" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer">Samuel Pun</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531630005" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3531808372" data-role="relative-time" class="time-ago" title="Sunday, September 24, 2017 2:30 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for the reminder! I think that is gradient check. I shall give it a try! thank you so much!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-2" data-action="upvote" title="" name="Vote up"><span class="updatable count" data-role="likes">2</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3531808372"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper "><a class="show-children" id="post-3531808372-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3531630005-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3531385013-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3525944498"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3525944498"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/roeibahumi/" data-action="profile" data-username="roeibahumi" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="265730883" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/roeibahumi/" data-action="profile" data-username="roeibahumi" target="_blank" rel="noopener noreferrer">ROEI Bahumi</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3525944498" data-role="relative-time" class="time-ago" title="Wednesday, September 20, 2017 5:21 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Great blog post. Thank you!!!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3525944498"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3525949089"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3525949089"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3525944498" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> ROEI Bahumi</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3525949089" data-role="relative-time" class="time-ago" title="Wednesday, September 20, 2017 5:28 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3525949089"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3525949089-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3525944498-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3523460449"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3523460449"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_TaZmcVBYfi/" data-action="profile" data-username="disqus_TaZmcVBYfi" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="135568488" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_TaZmcVBYfi/" data-action="profile" data-username="disqus_TaZmcVBYfi" target="_blank" rel="noopener noreferrer">Евгений Артеменко</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3523460449" data-role="relative-time" class="time-ago" title="Tuesday, September 19, 2017 4:02 AM">2 years ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Also, why do we use squre root of variance? Why don't we divide by std+epsilon ?<br>numpy even has std in it...</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3523460449"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3524528765"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3524528765"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3523460449" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Евгений Артеменко</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3524528765" data-role="relative-time" class="time-ago" title="Tuesday, September 19, 2017 10:03 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I guess I don't get your question. If you compare the formulas from the first image with the graph you should see that it's basically the same. Also this is not about what numpy has or has not, it's about understanding of the gradient flow and how you can disassemble any algorithm into small pieces you can differentiate and chain together by the chain rule.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3524528765"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3524528765-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3523460449-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3523454439"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3523454439"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_TaZmcVBYfi/" data-action="profile" data-username="disqus_TaZmcVBYfi" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="135568488" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_TaZmcVBYfi/" data-action="profile" data-username="disqus_TaZmcVBYfi" target="_blank" rel="noopener noreferrer">Евгений Артеменко</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3523454439" data-role="relative-time" class="time-ago" title="Tuesday, September 19, 2017 3:59 AM">2 years ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container" style="height: 374px;"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I don't understand dimensions in the first computational graph.<br>Do you have training examples in column-vectors or row-vectors?<br>Usually we have notation Rows x Columns for matrices:<br>Does D (number of columns) here correspond to m = training examples in mini-batch?<br>and N (number of rows) here correspond to n = activations in layer ? <br><span></span><div class="media-container media-mode-deferred">
<a class="media-button media-button-expand publisher-color publisher-border-color" href="https://uploads.disquscdn.com/images/c1315967b3075442fd39b0f53fd7dc76bcffcb02ebdfd53e7cce94300cc27ed3.png" rel="nofollow" target="_blank" data-action="expand" title="">
<i class="icon-images publisher-background-color"></i>
View
</a>
<a class="media-button media-button-contract publisher-color publisher-border-color" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" target="_blank" data-action="contract">
<i class="icon-cancel publisher-background-color"></i> Hide
</a>
<div class="media-content-loader" data-role="content-loader"></div>
<div data-role="content-placeholder" class="media-content-placeholder media-Disquscdn " style="height: 272px;"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="media-force-load" data-action="force-load"><i class="icon-images"></i></a>
</div>
</div><br>And why do we summing over N? Don't we need to sum over training examples , like this:<br><span></span><div class="media-container media-mode-deferred">
<a class="media-button media-button-expand publisher-color publisher-border-color" href="https://uploads.disquscdn.com/images/dd1833dd3e0ae4349d25f0bf0797ac73e614b5a4d3aadf380a81fbf97da3373a.png" rel="nofollow" target="_blank" data-action="expand" title="">
<i class="icon-images publisher-background-color"></i>
View
</a>
<a class="media-button media-button-contract publisher-color publisher-border-color" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" target="_blank" data-action="contract">
<i class="icon-cancel publisher-background-color"></i> Hide
</a>
<div class="media-content-loader" data-role="content-loader"></div>
<div data-role="content-placeholder" class="media-content-placeholder media-Disquscdn " style="height: 557px;"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="media-force-load" data-action="force-load"><i class="icon-images"></i></a>
</div>
</div></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3523454439"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3524524166"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3524524166"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="196251915" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/fkratzert/" data-action="profile" data-username="fkratzert" target="_blank" rel="noopener noreferrer">fkratzert</a></span> <span class="badge moderator">Mod</span></span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3523454439" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Евгений Артеменко</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3524524166" data-role="relative-time" class="time-ago" title="Tuesday, September 19, 2017 9:59 PM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>The matrix dimensions are row x columns as usual, so N is the number of training examples in the mini batch and D is the number of features. So it makes perfectly sense to sum over the N dimension, because batch normalization happens feature wise.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3524524166"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-3524853026"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3524853026"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_TaZmcVBYfi/" data-action="profile" data-username="disqus_TaZmcVBYfi" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="135568488" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_TaZmcVBYfi/" data-action="profile" data-username="disqus_TaZmcVBYfi" target="_blank" rel="noopener noreferrer">Евгений Артеменко</a></span> </span><span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3524524166" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> fkratzert</a></span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3524853026" data-role="relative-time" class="time-ago" title="Wednesday, September 20, 2017 1:42 AM">2 years ago</a> <span><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <span class="has-edit" data-role="has-edit">edited</span></span></span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>we have this as an input:<br><a href="https://uploads.disquscdn.com/images/81ee548150ece92fde048d1fe4086b9591cb0e44dbfe3802b7e455a89b24f490.png" rel="nofollow noopener" title="https://uploads.disquscdn.com/images/81ee548150ece92fde048d1fe4086b9591cb0e44dbfe3802b7e455a89b24f490.png">https://uploads.disquscdn.c...</a></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3524853026"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3524853026-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper "><a class="show-children" id="post-3524524166-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3523454439-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3517824785"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><ul class="post-menu dropdown" data-role="menu" data-view-id="post-menu" data-post-id="3517824785"><li class="collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class="" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer" class="user"><img data-role="user-avatar" data-user="165366918" src="./noavatar92.png" alt="Avatar"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/samuelpun/" data-action="profile" data-username="samuelpun" target="_blank" rel="noopener noreferrer">Samuel Pun</a></span> </span></span> <span class="post-meta"><span class="bullet time-ago-bullet" aria-hidden="true">•</span> <a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html#comment-3517824785" data-role="relative-time" class="time-ago" title="Friday, September 15, 2017 5:50 AM">2 years ago</a> </span>  </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thank you very much for this article. I learn a lot from you.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="updatable count" data-role="likes">0</span> <span class="control"><i aria-hidden="true" class="icon icon-arrow-2"></i></span></a> <a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" role="button" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"><i aria-hidden="true" class="icon icon-arrow"></i></span></a></div></li><li class="bullet" aria-hidden="true">•</li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li class="bullet" aria-hidden="true">•</li><li class="comment__share"><a class="toggle" tabindex="0"><span class="text">Share ›</span></a><ul class="comment-share__buttons"><li class="twitter"><button class="share__button" data-action="share:twitter">Twitter</button></li><li class="facebook"><button class="share__button" data-action="share:facebook">Facebook</button></li><li class="link"><input class="share__button" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3517824785"><span class="realtime-replies" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" class="realtime-button" style="display: none;"></a></li></menu></footer></div><div data-role="blacklist-form"></div><div data-role="flagging-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3517824785-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="load-more" data-role="more" style=""><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" data-action="more-posts" class="btn load-more__button">Load more comments</a></div></div></section><div id="placement-bottom" data-tracking-area="discovery-south"><div class="post-list" style="height: auto; visibility: visible;"><div style="display: block; width: 100%;"><div id="discovery-main-c260" class="discovery-main"><section id="col-organic-c501" class="col-organic"><header class="discovery-col-header"><h2>Also on <strong>kratzertblog</strong></h2></header><ul class="discovery-posts" data-role="discovery-posts"><li class="discovery-post post-0" id="discovery-link-organic-6137751402"><a class="publisher-anchor-color" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F09%2F12%2Fintroduction-to-the-numba-library.html&amp;key=Q73Bk6ANQXwjAC8nupDHRQ" target="" rel=""><header class="discovery-post-header"><h3 title="Introduction to the Numba library"><span data-role="discovery-thread-title" class="title line-truncate" data-line-truncate="2">Introduction to the Numba library</span></h3><ul class="meta"><li class="comments">6 comments </li> <li class="time">2 years ago</li></ul></header><a class="top-comment" data-role="discovery-top-comment" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F09%2F12%2Fintroduction-to-the-numba-library.html&amp;key=Q73Bk6ANQXwjAC8nupDHRQ" target="" rel=""><img alt="Avatar" data-role="discovery-avatar"><p><span class="user" data-role="discovery-top-comment-author">Camille Moatti</span> — <span data-role="discovery-top-comment-snippet" class="line-truncate" data-line-truncate="3">Thanks a lot for your article. It is very well explained.</span></p></a></a></li><li class="discovery-post post-1" id="discovery-link-organic-5914010462" style="margin-bottom: 20px;"><a class="publisher-anchor-color" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F06%2F15%2Fexample-of-tensorflows-new-input-pipeline.html&amp;key=eLW9n1InH-7UKCOGmUbvxw" target="" rel=""><header class="discovery-post-header"><h3 title="Example of TensorFlows new Input Pipeline"><span data-role="discovery-thread-title" class="title line-truncate" data-line-truncate="2">Example of TensorFlows new Input Pipeline</span></h3><ul class="meta"><li class="comments">46 comments </li> <li class="time">2 years ago</li></ul></header><a class="top-comment" data-role="discovery-top-comment" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F06%2F15%2Fexample-of-tensorflows-new-input-pipeline.html&amp;key=eLW9n1InH-7UKCOGmUbvxw" target="" rel=""><img alt="Avatar" data-role="discovery-avatar"><p><span class="user" data-role="discovery-top-comment-author">fkratzert</span> — <span data-role="discovery-top-comment-snippet" class="line-truncate" data-line-truncate="3"> At the link below you will find a solution how to evaluate your trained network. I'm not sure if this is still the most "modern" TF …</span></p></a></a></li><li class="discovery-post post-2" id="discovery-link-organic-6135464386"><a class="publisher-anchor-color" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F09%2F11%2Fspeeding-up-tensorflows-input-pipeline.html&amp;key=C98tcbMgrqreIIDAU5qloA" target="" rel=""><header class="discovery-post-header"><h3 title="Speeding up TensorFlows Input Pipeline"><span data-role="discovery-thread-title" class="title line-truncate" data-line-truncate="2">Speeding up TensorFlows Input Pipeline</span></h3><ul class="meta"><li class="comments">5 comments </li> <li class="time">2 years ago</li></ul></header><a class="top-comment" data-role="discovery-top-comment" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F09%2F11%2Fspeeding-up-tensorflows-input-pipeline.html&amp;key=C98tcbMgrqreIIDAU5qloA" target="" rel=""><img alt="Avatar" data-role="discovery-avatar"><p><span class="user" data-role="discovery-top-comment-author">wogong</span> — <span data-role="discovery-top-comment-snippet" class="line-truncate" data-line-truncate="3"> Sorry for my off topic. I find this blog from your wonderful repo finetune_alexnet_with_tensorflow, recently I …</span></p></a></a></li><li class="discovery-post post-3" id="discovery-link-organic-5581251971" style="margin-bottom: 20px;"><a class="publisher-anchor-color" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F02%2F24%2Ffinetuning-alexnet-with-tensorflow.html&amp;key=cFMiDsxyTE9QfLNmwwGMJg" target="" rel=""><header class="discovery-post-header"><h3 title="Finetuning AlexNet with TensorFlow"><span data-role="discovery-thread-title" class="title line-truncate" data-line-truncate="2">Finetuning AlexNet with TensorFlow</span></h3><ul class="meta"><li class="comments">191 comments </li> <li class="time">2 years ago</li></ul></header><a class="top-comment" data-role="discovery-top-comment" href="https://disq.us/?url=https%3A%2F%2Fkratzert.github.io%2F2017%2F02%2F24%2Ffinetuning-alexnet-with-tensorflow.html&amp;key=cFMiDsxyTE9QfLNmwwGMJg" target="" rel=""><img alt="Avatar" data-role="discovery-avatar"><p><span class="user" data-role="discovery-top-comment-author">Novian Adi Prasetyo</span> — <span data-role="discovery-top-comment-snippet" class="line-truncate" data-line-truncate="3">same problem, do you found solution?</span></p></a></a></li></ul></section></div></div></div></div><div id="footer" data-tracking-area="footer" class="disqus-footer__wrapper"><ul class="disqus-footer"><li class="disqus-footer__logo"><a href="https://disqus.com/" rel="nofollow" title="Powered by Disqus" class="disqus-footer__link">Powered by Disqus</a></li><li id="thread-subscribe-button" class="email disqus-footer__item"><div class="default"><a href="https://disqus.com/embed/comments/?base=default&amp;f=kratzertblog&amp;t_u=https%3A%2F%2Fkratzert.github.io%2F2016%2F02%2F12%2Funderstanding-the-gradient-flow-through-the-batch-normalization-layer.html&amp;t_d=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;t_t=Understanding%20the%20backward%20pass%20through%20Batch%20Normalization%20Layer&amp;s_o=default#" rel="nofollow" data-action="subscribe" title="Subscribe and get email updates from this discussion" class="disqus-footer__link"><i aria-hidden="true" class="icon icon-mail"></i><span class="clip">Subscribe</span><i aria-hidden="true" class="icon icon-checkmark"></i></a></div></li><li class="install disqus-footer__item"><a href="https://publishers.disqus.com/engage?utm_source=kratzertblog&amp;utm_medium=Disqus-Footer" rel="nofollow" target="_blank" class="disqus-footer__link"><i aria-hidden="true" class="icon icon-disqus"></i><span class="clip hidden-md">Add Disqus to your site</span><span class="clip visible-md hidden-xs">Add Disqus</span><span class="clip visible-xs">Add</span></a></li><li class="privacy disqus-footer__item"><a href="https://help.disqus.com/customer/portal/articles/466259-privacy-policy" rel="nofollow" target="_blank" class="disqus-footer__link"><i aria-hidden="true" class="icon icon-lock"></i><span class="clip hidden-sm">Disqus' Privacy Policy</span><span class="clip visible-sm hidden-xs">Privacy Policy</span><span class="clip visible-xs">Privacy</span></a></li></ul></div></div><div id="fb-root" class=" fb_reset"><div style="position: absolute; top: -10000px; width: 0px; height: 0px;"><div></div><div><iframe name="fb_xdm_frame_https" id="fb_xdm_frame_https" aria-hidden="true" title="Facebook Cross Domain Communication Frame" tabindex="-1" frameborder="0" allowtransparency="true" allowfullscreen="true" scrolling="no" allow="encrypted-media" src="./xd_arbiter.html" style="border: none;"></iframe></div></div></div><iframe id="ssIFrame_google" sandbox="allow-scripts allow-same-origin" aria-hidden="true" frame-border="0" src="./iframe.html" style="position: absolute; width: 1px; height: 1px; left: -9999px; top: -9999px; right: -9999px; bottom: -9999px; display: none;"></iframe><iframe src="./pixel.html" style="display: none;"></iframe><img src="./saved_resource" style="display: none;"><iframe src="./sync.html" style="display: none;"></iframe></body></html>